{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration from config.json\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pyarrow.parquet as pq\n",
    "import time\n",
    "from utils import config\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sankey import Sankey\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import tableone\n",
    "try:\n",
    "    from tableone import TableOne, load_dataset\n",
    "except (ModuleNotFoundError, ImportError):\n",
    "    # install on Colab\n",
    "    !pip install tableone\n",
    "    from tableone import TableOne, load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## please change site_name, tables_path, output_path, and file_type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access configuration parameters\n",
    "site_name = config['site_name']\n",
    "tables_path = config['tables_path']\n",
    "file_type = config['file_type']\n",
    "output_path = os.path.join(\"..\", \"output\", \"final\")\n",
    "\n",
    "# Make sure the directory exists; if not, create it\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Print the configuration parameters\n",
    "print(f\"Site Name: {site_name}\")\n",
    "print(f\"Tables Path: {tables_path}\")\n",
    "print(f\"Output path: {output_path}\")\n",
    "print(f\"File Type: {file_type}\")\n",
    "                   \n",
    "## Confirm that these are the correct paths\n",
    "adt_filepath = f\"{tables_path}/clif_adt.{file_type}\"\n",
    "hospitalization_filepath = f\"{tables_path}/clif_hospitalization.{file_type}\"\n",
    "patient_filepath = f\"{tables_path}/clif_patient.{file_type}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath, filetype):\n",
    "    start_time = time.time()  # Record the start time\n",
    "    file_name = os.path.basename(filepath) \n",
    "    if filetype == 'csv':\n",
    "        df = pd.read_csv(filepath)\n",
    "    elif filetype == 'parquet':\n",
    "        table = pq.read_table(filepath)\n",
    "        df = table.to_pandas()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please provide either 'csv' or 'parquet'.\")\n",
    "    \n",
    "    end_time = time.time()  # Record the end time\n",
    "    load_time = end_time - start_time  # Calculate the loading time\n",
    "    \n",
    "    # Calculate the size of the loaded dataset in MB\n",
    "    dataset_size_mb = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "    print(f\"File name: {file_name}\")\n",
    "    print(f\"Time taken to load the dataset: {load_time:.2f} seconds\")\n",
    "    print(f\"Size of the loaded dataset: {dataset_size_mb:.2f} MB\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "adt = read_data(adt_filepath, file_type)\n",
    "hosp = read_data(hospitalization_filepath, file_type)\n",
    "patient = read_data(patient_filepath,file_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of unique hospitalization_id:\",hosp[\"hospitalization_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_admission_dttm = hosp['admission_dttm'].isnull().sum()\n",
    "print(\"Missing admission_dttm:\", missing_admission_dttm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp2 = hosp[[\"patient_id\",\"hospitalization_id\",\"admission_dttm\",\"discharge_dttm\",\"age_at_admission\"]].copy()\n",
    "hosp2['admission_dttm'] = pd.to_datetime(hosp2['admission_dttm'])\n",
    "hosp2['discharge_dttm'] = pd.to_datetime(hosp2['discharge_dttm'])\n",
    "hosp2 = hosp2[(hosp2['admission_dttm'].dt.year >= 2020) & \n",
    "                   (hosp2['admission_dttm'].dt.year <= 2021) & \n",
    "                   (hosp2['age_at_admission'] >=18)&\n",
    "                    (hosp2['age_at_admission'] <=119)]\n",
    "\n",
    "df = pd.merge(hosp2[[\"patient_id\",\"hospitalization_id\"]],\n",
    "                  adt[[\"hospitalization_id\",\"in_dttm\",\"out_dttm\",\"location_category\"]],\n",
    "             on=\"hospitalization_id\",how=\"left\")\n",
    "\n",
    "print(\"number of unique hospitalization_id:\",df[\"hospitalization_id\"].nunique())\n",
    "print(\"number of rows in df:\",len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"location_category\"] = df[\"location_category\"].str.lower().str.strip()\n",
    "# Mapping dictionary to update the location_category\n",
    "location_mapping = {\n",
    "    'rehab': 'Other',\n",
    "    'l&d': 'Ward',\n",
    "    'psych': 'Other',\n",
    "    'radiology': 'Other',\n",
    "    'dialysis': 'Other',\n",
    "    'hospice': 'Other',\n",
    "    'stepdown':'ICU',\n",
    "    \"icu\": \"ICU\",\n",
    "    \"er\": \"ER\",\n",
    "    \"ward\":\"Ward\",\n",
    "    \"procedural\":\"Procedural\",\n",
    "    \"other\":\"Other\"\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'location_category' column\n",
    "df['location_category'] = df['location_category'].replace(location_mapping)\n",
    "df[\"location_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sort by hospitalization_id and in_dttm\n",
    "df = df.sort_values(by=['hospitalization_id', 'in_dttm', 'out_dttm'])\n",
    "df = df[df['location_category']!= \"Other\"] #removing \"Other from the study\"\n",
    "\n",
    "# Step 2: Group by hospitalization_id and apply the removal logic\n",
    "def remove_procedural_pattern(group):\n",
    "    # Loop through the group to find the ICU --> Procedural --> ICU pattern\n",
    "    to_remove = []\n",
    "    \n",
    "    for i in range(1, len(group) - 1):  # Start from second row and end before the last row\n",
    "        # Check for ICU -> Procedural -> ICU pattern\n",
    "        if (group.iloc[i-1]['location_category'] == 'ICU' and\n",
    "            group.iloc[i]['location_category'] == 'Procedural' and\n",
    "            group.iloc[i+1]['location_category'] == 'ICU'):\n",
    "            # Mark the 'Procedural' row for removal\n",
    "            to_remove.append(group.index[i])\n",
    "    \n",
    "    # Return the group without the rows to be removed\n",
    "    return group.drop(to_remove)\n",
    "\n",
    "# Apply the function to each group of hospitalization_id\n",
    "df = df.sort_values(by=['hospitalization_id', 'in_dttm', 'out_dttm'])\n",
    "#stitch together ICUs if ICU --> procedure --> ICU\n",
    "df['shifted_location'] = df['location_category'].shift(1)\n",
    "df['new_location'] = (df['location_category'] != df['shifted_location']).cumsum()\n",
    "df = df.groupby(['hospitalization_id','new_location']).agg(\n",
    "    in_dttm=pd.NamedAgg(column='in_dttm', aggfunc='min'),\n",
    "    out_dttm=pd.NamedAgg(column='out_dttm', aggfunc='max'),\n",
    "    location_category=pd.NamedAgg(column='location_category', aggfunc='first'), \n",
    "    patient_id=pd.NamedAgg(column='patient_id', aggfunc='first')\n",
    ").reset_index()\n",
    "df = df.groupby('hospitalization_id').apply(remove_procedural_pattern).reset_index(drop=True)\n",
    "\n",
    "print(\"Number of unique hospitalization_id:\", df[\"hospitalization_id\"].nunique())\n",
    "print(\"Number of rows in df:\",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_stays(df):\n",
    "    # Ensure that the in_dttm and out_dttm columns are in datetime format\n",
    "    df['in_dttm'] = pd.to_datetime(df['in_dttm'])\n",
    "    df['out_dttm'] = pd.to_datetime(df['out_dttm'])\n",
    "    \n",
    "    # Calculate the time difference between out_dttm and in_dttm\n",
    "    df['stay_duration'] = df['out_dttm'] - df['in_dttm']\n",
    "    \n",
    "    # Filter out rows where the stay duration is less than 1 hour\n",
    "    df_filtered = df[df['stay_duration'] >= pd.Timedelta(hours=1)]\n",
    "    \n",
    "    # Drop the stay_duration column as it is no longer needed\n",
    "    df_filtered = df_filtered.drop(columns=['stay_duration'])\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# Sort the DataFrame as needed\n",
    "df = df.sort_values(by=['hospitalization_id', 'in_dttm', 'out_dttm'])\n",
    "\n",
    "# Apply the function to the entire DataFrame, not with .apply()\n",
    "df = remove_short_stays(df)\n",
    "\n",
    "# Perform further transformations\n",
    "df['shifted_location'] = df['location_category'].shift(1)\n",
    "df['new_location'] = (df['location_category'] != df['shifted_location']).cumsum()\n",
    "\n",
    "# Group by hospitalization_id and new_location, and aggregate\n",
    "df = df.groupby(['hospitalization_id', 'new_location']).agg(\n",
    "    in_dttm=pd.NamedAgg(column='in_dttm', aggfunc='min'),\n",
    "    out_dttm=pd.NamedAgg(column='out_dttm', aggfunc='max'),\n",
    "    location_category=pd.NamedAgg(column='location_category', aggfunc='first'),\n",
    "    patient_id=pd.NamedAgg(column='patient_id', aggfunc='first')\n",
    ").reset_index()\n",
    "\n",
    "# Print summary information\n",
    "print(\"Number of unique hospitalization_id:\", df[\"hospitalization_id\"].nunique())\n",
    "print(\"Number of rows in df:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['in_dttm'] = pd.to_datetime(df['in_dttm'])\n",
    "df['out_dttm'] = pd.to_datetime(df['out_dttm'])\n",
    "df = df.sort_values(by=['hospitalization_id', 'in_dttm', 'out_dttm'])\n",
    "\n",
    "def mark_procedural_before_second_icu(df):\n",
    "    # Initialize the 'procedural' column with False\n",
    "    df['procedural'] = False\n",
    "\n",
    "    # Function to process each group of 'patient_id' and 'hospitalization_id'\n",
    "    def process_group(group):\n",
    "        # Find all the rows where the location is ICU\n",
    "        icu_indices = group[group['location_category'] == 'ICU'].index\n",
    "\n",
    "        # Check if at least two ICU admissions exist in the group\n",
    "        if len(icu_indices) > 1:\n",
    "            # Get the index of the second ICU admission\n",
    "            second_icu_index = icu_indices[1]\n",
    "\n",
    "            # Check the row just before the second ICU admission\n",
    "            if second_icu_index > group.index[0]:\n",
    "                row_before_second_icu = group.iloc[second_icu_index - group.index[0] - 1]\n",
    "\n",
    "                # If the row before the second ICU is \"Procedural\", mark the entire group as procedural\n",
    "                if row_before_second_icu['location_category'] == 'Procedural':\n",
    "                    df.loc[group.index, 'procedural'] = True  # Mark the entire group as procedural\n",
    "\n",
    "    # Apply the function to each group of 'patient_id' and 'hospitalization_id'\n",
    "    df.groupby(['patient_id', 'hospitalization_id']).apply(process_group)\n",
    "\n",
    "    return df\n",
    "\n",
    "df2 = df.sort_values(by=['hospitalization_id', 'in_dttm', 'out_dttm'])\n",
    "#making sure the dataset is aggregated correctly\n",
    "df2['shifted_location'] = df2['location_category'].shift(1)\n",
    "df2['new_location'] = (df2['location_category'] != df2['shifted_location']).cumsum()\n",
    "df2 = df2.groupby(['hospitalization_id','new_location']).agg(\n",
    "    in_dttm=pd.NamedAgg(column='in_dttm', aggfunc='min'),\n",
    "    out_dttm=pd.NamedAgg(column='out_dttm', aggfunc='max'),\n",
    "    location_category=pd.NamedAgg(column='location_category', aggfunc='first'), \n",
    "    patient_id=pd.NamedAgg(column='patient_id', aggfunc='first')\n",
    ").reset_index()\n",
    "df2 = mark_procedural_before_second_icu(df2)\n",
    "\n",
    "# Output the updated DataFrame\n",
    "print(\"Number of unique hospitalization_id:\", df2[\"hospitalization_id\"].nunique())\n",
    "print(\"Number of rows in df:\", len(df2))\n",
    "print(\"Number of hospitalizations marked as procedural:\", df2[df2[\"procedural\"]==True][\"hospitalization_id\"].nunique())\n",
    "df2[['patient_id', 'hospitalization_id', 'location_category', 'procedural']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['in_dttm'] = pd.to_datetime(df2['in_dttm'])\n",
    "df2['out_dttm'] = pd.to_datetime(df2['out_dttm'])\n",
    "df2 = df2.sort_values(by=['hospitalization_id', 'in_dttm', 'out_dttm'])\n",
    "\n",
    "# Function to filter visit_occurrence_ids based on ICU location\n",
    "def filter_icu_visits(df):\n",
    "    # Find all rows where location is ICU\n",
    "    icu_visits = df[df['location_category'] == \"ICU\"]\n",
    "\n",
    "    # If there are no ICU visits, return an empty DataFrame\n",
    "    if icu_visits.empty:\n",
    "        return pd.DataFrame(columns=df.columns)\n",
    "    \n",
    "    # Get the unique visit_occurrence_ids that include ICU\n",
    "    icu_visit_ids = icu_visits['hospitalization_id'].unique()\n",
    "    \n",
    "    # Filter to include only rows with these visit_occurrence_ids (all ICU visits)\n",
    "    df_filtered = df[df['hospitalization_id'].isin(icu_visit_ids)]\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# Apply the function to each visit occurence to keep all ICU visits\n",
    "icu_df = df2.groupby('hospitalization_id').apply(filter_icu_visits).reset_index(drop=True)\n",
    "#icu_df = icu_df[icu_df['location_category'] != \"Procedural\"]\n",
    "icu_df = icu_df.sort_values(by=['hospitalization_id', 'in_dttm','out_dttm'])\n",
    "\n",
    "print(\"Number of unique hospitalization_id:\", icu_df[\"hospitalization_id\"].nunique())\n",
    "print(\"Number of rows in df:\",len(icu_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_deaths_during_icu(df, patient_df):\n",
    "    # Merge death_dttm from patient data into icu_df\n",
    "    df = pd.merge(df, patient_df[[\"patient_id\", \"death_dttm\"]], on=\"patient_id\", how=\"left\")\n",
    "\n",
    "    # Ensure the columns 'in_dttm' and 'out_dttm' are in datetime format\n",
    "    df['death_dttm'] = pd.to_datetime(df['death_dttm'], errors='coerce')\n",
    "    df['in_dttm'] = pd.to_datetime(df['in_dttm'], errors='coerce')\n",
    "    df['out_dttm'] = pd.to_datetime(df['out_dttm'], errors='coerce')\n",
    "\n",
    "    # Sort the DataFrame by hospitalization_id and in_dttm for correct processing\n",
    "    df = df.sort_values(by=['hospitalization_id', 'in_dttm'])\n",
    "\n",
    "    # Identify first ICU admission per hospitalization\n",
    "    icu_first_admission = df[df['location_category'] == 'ICU'].groupby('hospitalization_id').first().reset_index()\n",
    "\n",
    "    # Identify hospitalization_id where the patient died during ICU admission\n",
    "    death_during_icu = icu_first_admission[\n",
    "        (icu_first_admission['death_dttm'].notnull()) &\n",
    "        (icu_first_admission['death_dttm'] >= icu_first_admission['in_dttm']) &\n",
    "        (icu_first_admission['death_dttm'] <= icu_first_admission['out_dttm'])\n",
    "    ]['hospitalization_id']\n",
    "\n",
    "    # Drop rows where the patient died during ICU admission\n",
    "    dropped_df = df[df['hospitalization_id'].isin(death_during_icu)]\n",
    "    df_cleaned = df[~df['hospitalization_id'].isin(death_during_icu)]\n",
    "\n",
    "    return df_cleaned, dropped_df\n",
    "\n",
    "icu_df = icu_df.sort_values(by=['hospitalization_id', 'in_dttm'])\n",
    "icu_df2, dropped_deaths_df = drop_deaths_during_icu(icu_df, patient)\n",
    "\n",
    "print(\"Number of unique hospitalization_id with death during ICU :\",dropped_deaths_df[\"hospitalization_id\"].nunique())\n",
    "print(\"Number of unique hospitalization_id after removing death during ICU:\", icu_df2[\"hospitalization_id\"].nunique())\n",
    "print(\"Number of rows in df:\",len(icu_df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_direct_icu_discharges(df):\n",
    "    # Initialize an empty list to hold custom_visit_occurrence2 values to be dropped\n",
    "    visits_to_drop = []\n",
    "\n",
    "    # Function to process each group of 'person_id' and 'custom_visit_occurrence2'\n",
    "    def process_group(group):\n",
    "        # Find all the rows where the location is ICU\n",
    "        icu_indices = group[group['location_category'] == 'ICU'].index\n",
    "\n",
    "        # Check if any ICU admissions exist in the group\n",
    "        if not icu_indices.empty:\n",
    "            # Get the index of the first ICU admission\n",
    "            first_icu_index = icu_indices[0]\n",
    "\n",
    "            # Check if this first ICU admission is also the last row in the visit\n",
    "            if first_icu_index == group.index[-1]:\n",
    "                # This means the patient was discharged or died directly from the ICU\n",
    "                visits_to_drop.append(group.iloc[0]['hospitalization_id'])  # Mark the visit to be dropped\n",
    "\n",
    "    # Apply the function to each group of 'person_id' and 'custom_visit_occurrence2'\n",
    "    df.groupby(['patient_id', 'hospitalization_id']).apply(process_group)\n",
    "\n",
    "    # Create a new DataFrame with the rows that need to be dropped\n",
    "    dropped_df = df[df['hospitalization_id'].isin(visits_to_drop)]\n",
    "\n",
    "    # Drop the visits from the original DataFrame\n",
    "    df_cleaned = df[~df['hospitalization_id'].isin(visits_to_drop)]\n",
    "\n",
    "    return df_cleaned, dropped_df\n",
    "\n",
    "icu_df2 = icu_df2.sort_values(by=['hospitalization_id', 'in_dttm','out_dttm'])\n",
    "icu_df3, dropped_df_icu_discharge = drop_direct_icu_discharges(icu_df2)\n",
    "\n",
    "# Output the counts and dropped DataFrame\n",
    "print(\"Number of unique hospitalization_id after filtering:\", icu_df3[\"hospitalization_id\"].nunique())\n",
    "print(\"Number of rows in df:\",len(icu_df3))\n",
    "print(\"Number of unique patient_id dropped:\", dropped_df_icu_discharge[\"patient_id\"].nunique())\n",
    "print(\"Number of unique hospitalization_id dropped:\", dropped_df_icu_discharge[\"hospitalization_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redundant but making sure that the location_category is grouped together \n",
    "icu_df4 = icu_df3.sort_values(by=['hospitalization_id', 'in_dttm','out_dttm'])\n",
    "icu_df4['shifted_location'] = icu_df4['location_category'].shift(1)\n",
    "icu_df4['new_location'] = (icu_df4['location_category'] != icu_df4['shifted_location']).cumsum()\n",
    "\n",
    "icu_df4 = icu_df4.groupby(['hospitalization_id','new_location']).agg(\n",
    "    earliest_location_start=pd.NamedAgg(column='in_dttm', aggfunc='min'),\n",
    "    latest_location_end=pd.NamedAgg(column='out_dttm', aggfunc='max'),\n",
    "    location_category=pd.NamedAgg(column='location_category', aggfunc='first'), \n",
    "    patient_id=pd.NamedAgg(column='patient_id', aggfunc='first'),\n",
    "    procedural = pd.NamedAgg(column='procedural', aggfunc='first')\n",
    ").reset_index()\n",
    "\n",
    "print(\"Number of unique hospitalization_id:\", icu_df4[\"hospitalization_id\"].nunique())\n",
    "icu_df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_intermediate_er(df):\n",
    "    # Initialize an empty list to hold hospitalization_id values to be dropped\n",
    "    visits_to_drop = []\n",
    "\n",
    "    # Function to process each group of 'patient_id' and 'hospitalization_id'\n",
    "    def process_group(group):\n",
    "        # Check if 'ER' appears after the first row in the group\n",
    "        if any(group.iloc[1:]['location_category'] == 'ER'):\n",
    "            # Mark the visit for dropping if 'ER' appears after the first row\n",
    "            visits_to_drop.append(group.iloc[0]['hospitalization_id'])\n",
    "\n",
    "    # Apply the function to each group of'hospitalization_id'\n",
    "    df.groupby(['hospitalization_id']).apply(process_group)\n",
    "\n",
    "    # Create a new DataFrame with the rows that need to be dropped\n",
    "    dropped_df = df[df['hospitalization_id'].isin(visits_to_drop)]\n",
    "\n",
    "    # Drop the visits from the original DataFrame\n",
    "    df_cleaned = df[~df['hospitalization_id'].isin(visits_to_drop)]\n",
    "\n",
    "    return df_cleaned, dropped_df\n",
    "\n",
    "# Sort and apply the drop function\n",
    "icu_df4 = icu_df4.sort_values(by=['hospitalization_id', 'earliest_location_start', 'latest_location_end'])\n",
    "icu_df5, dropped_df = drop_intermediate_er(icu_df4)\n",
    "\n",
    "# Convert to datetime and calculate location hours\n",
    "icu_df5['earliest_location_start'] = pd.to_datetime(icu_df5['earliest_location_start'])\n",
    "icu_df5['latest_location_end'] = pd.to_datetime(icu_df5['latest_location_end'])\n",
    "icu_df5['location_hours'] = (icu_df5['latest_location_end'] - icu_df5['earliest_location_start']).dt.total_seconds() / 3600\n",
    "\n",
    "# Output the results\n",
    "print(\"Number of rows in df:\", len(icu_df5))\n",
    "print(\"Number of unique hospitalization_id:\", icu_df5[\"hospitalization_id\"].nunique())\n",
    "icu_df5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating the readmission flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_icu_readmission_time(df):\n",
    "    # Initialize a new column 'ICU_readmission_hour' with None\n",
    "    df['ICU_readmission_hour'] = None\n",
    "\n",
    "    # Function to process each group\n",
    "    def process_group(group):\n",
    "        # Find the rows where location_category is ICU\n",
    "        icu_rows = group[group['location_category'] == 'ICU']\n",
    "\n",
    "        # Check if there are at least two ICU events\n",
    "        if len(icu_rows) > 1:\n",
    "            # Sort by earliest_start2 to ensure correct order\n",
    "            icu_rows = icu_rows.sort_values(by='earliest_location_start')\n",
    "\n",
    "            # Get the first and second ICU event\n",
    "            first_icu = icu_rows.iloc[0]\n",
    "            second_icu = icu_rows.iloc[1]\n",
    "\n",
    "            # Calculate the time difference between first and second ICU events in hours\n",
    "            time_diff = (second_icu['earliest_location_start'] - first_icu['latest_location_end']).total_seconds() / 3600\n",
    "\n",
    "            # Update the entire group with the calculated time difference\n",
    "            df.loc[group.index, 'ICU_readmission_hour'] = time_diff\n",
    "\n",
    "    # Apply the function to each group of 'person_id' and 'custom_visit_occurrence2'\n",
    "    df.groupby(['patient_id', 'hospitalization_id']).apply(process_group)\n",
    "\n",
    "    return df\n",
    "\n",
    "icu_df6 = calculate_icu_readmission_time(icu_df5)\n",
    "print(\"number of rows in df:\",len(icu_df6))\n",
    "print(\"Number of unique hospitalization_id:\", icu_df6[\"hospitalization_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify hospitalization_ids where ICU_readmission_hour is less than 0\n",
    "#This is in place incase there are errors in the adt, should be a very small number if any \n",
    "hosp_ids_to_drop = icu_df6[icu_df6[\"ICU_readmission_hour\"] <=0][\"hospitalization_id\"].unique()\n",
    "\n",
    "# Drop rows with those hospitalization_ids\n",
    "icu_df7 = icu_df6[~icu_df6[\"hospitalization_id\"].isin(hosp_ids_to_drop)]\n",
    "\n",
    "# Check the result\n",
    "print(\"Number of unique hospitalization_id after filtering:\", icu_df7[\"hospitalization_id\"].nunique())\n",
    "print(\"Number of rows in df after filtering:\", len(icu_df7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#icu_df6[icu_df6[\"ICU_readmission_hour\"]>0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns\n",
    "icu_final = icu_df7[[\"patient_id\", \"hospitalization_id\", \"ICU_readmission_hour\",\"procedural\"]]\n",
    "\n",
    "# Drop duplicates based on 'custom_visit_occurrence2'\n",
    "icu_final = icu_final.drop_duplicates(subset=[\"hospitalization_id\"])\n",
    "\n",
    "# Display the first few rows of the result\n",
    "print(\"Number of rows in df:\",len(icu_final))\n",
    "print(\"Number of unique patient_id:\", icu_final[\"patient_id\"].nunique())\n",
    "print(\"Number of unique hospitalization_id:\", icu_final[\"hospitalization_id\"].nunique())\n",
    "icu_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_unique_visits = icu_final[\"hospitalization_id\"].nunique()\n",
    "unique_visits_with_procedures = icu_final[icu_final[\"procedural\"]==True][\"hospitalization_id\"].nunique()\n",
    "unique_visits_without_procedures = icu_final[icu_final[\"procedural\"]==False][\"hospitalization_id\"].nunique()\n",
    "all_icu_readmission = icu_final[(icu_final[\"ICU_readmission_hour\"]>0)][\"hospitalization_id\"].nunique()\n",
    "all_icu_readmission_6hours = icu_final[(icu_final[\"ICU_readmission_hour\"]<6)][\"hospitalization_id\"].nunique()\n",
    "all_icu_readmission_24hours = icu_final[(icu_final[\"ICU_readmission_hour\"]<24)][\"hospitalization_id\"].nunique()\n",
    "all_icu_readmission_48hours = icu_final[(icu_final[\"ICU_readmission_hour\"]<48)][\"hospitalization_id\"].nunique()\n",
    "all_icu_readmission_72hours = icu_final[(icu_final[\"ICU_readmission_hour\"]<72)][\"hospitalization_id\"].nunique()\n",
    "unplanned_icu_readmission = icu_final[(icu_final[\"procedural\"]==False)&(icu_final[\"ICU_readmission_hour\"]>0)][\"hospitalization_id\"].nunique()\n",
    "unplanned_icu_readmission_6hours = icu_final[(icu_final[\"procedural\"]==False)&(icu_final[\"ICU_readmission_hour\"]<6)][\"hospitalization_id\"].nunique()\n",
    "unplanned_icu_readmission_24hours = icu_final[(icu_final[\"procedural\"]==False)&(icu_final[\"ICU_readmission_hour\"]<24)][\"hospitalization_id\"].nunique()\n",
    "unplanned_icu_readmission_48hours = icu_final[(icu_final[\"procedural\"]==False)&(icu_final[\"ICU_readmission_hour\"]<48)][\"hospitalization_id\"].nunique()\n",
    "unplanned_icu_readmission_72hours = icu_final[(icu_final[\"procedural\"]==False)&(icu_final[\"ICU_readmission_hour\"]<72)][\"hospitalization_id\"].nunique()\n",
    "\n",
    "\n",
    "print(\"Number of unique hospitalization_id:\", total_unique_visits)\n",
    "print(\"Planned readmission: Number of unique hospitalization_id with procedures between ICU readmission:\", unique_visits_with_procedures)\n",
    "print(\"Unplanned readmission: Number of unique hospitalization_id without any procedures between ICU readmission:\", unique_visits_without_procedures)\n",
    "\n",
    "all_icu_read = round((all_icu_readmission / total_unique_visits)*100,2)\n",
    "all_6hrs = round((all_icu_readmission_6hours/total_unique_visits)*100,2)\n",
    "all_24hrs = round((all_icu_readmission_24hours/total_unique_visits)*100,2)\n",
    "all_48hrs = round((all_icu_readmission_48hours/total_unique_visits)*100,2)\n",
    "all_72hrs = round((all_icu_readmission_72hours/total_unique_visits)*100,2)\n",
    "\n",
    "unplanned_icu_read = round((unplanned_icu_readmission / unique_visits_without_procedures)*100,2)\n",
    "unplanned_6hrs = round((unplanned_icu_readmission_6hours/unique_visits_without_procedures)*100,2)\n",
    "unplanned_24hrs = round((unplanned_icu_readmission_24hours/unique_visits_without_procedures)*100,2)\n",
    "unplanned_48hrs = round((unplanned_icu_readmission_48hours/unique_visits_without_procedures)*100,2)\n",
    "unplanned_72hrs = round((unplanned_icu_readmission_72hours/unique_visits_without_procedures)*100,2)\n",
    "\n",
    "# Print all readmission rate\n",
    "print(f\"All ICU readmission ({all_icu_readmission}/{total_unique_visits}): {all_icu_read}%\")\n",
    "print(f\"All ICU readmission <6hr ({all_icu_readmission_6hours}/{total_unique_visits}): {all_6hrs}%\")\n",
    "print(f\"All ICU readmission <24hr ({all_icu_readmission_24hours}/{total_unique_visits}): {all_24hrs}%\")\n",
    "print(f\"All ICU readmission <48hr ({all_icu_readmission_48hours}/{total_unique_visits}): {all_48hrs}%\")\n",
    "print(f\"All ICU readmission <72hr ({all_icu_readmission_72hours}/{total_unique_visits}): {all_72hrs}%\")\n",
    "\n",
    "# Print unplanned readmission rate\n",
    "print(f\"Unplanned ICU readmission ({unplanned_icu_readmission}/{unique_visits_without_procedures}): {unplanned_icu_read}%\")\n",
    "print(f\"Unplanned ICU readmission <6hr ({unplanned_icu_readmission_6hours}/{unique_visits_without_procedures}): {unplanned_6hrs}%\")\n",
    "print(f\"Unplanned ICU readmission <24hr ({unplanned_icu_readmission_24hours}/{unique_visits_without_procedures}): {unplanned_24hrs}%\")\n",
    "print(f\"Unplanned ICU readmission <48hr ({unplanned_icu_readmission_48hours}/{unique_visits_without_procedures}): {unplanned_48hrs}%\")\n",
    "print(f\"Unplanned ICU readmission <72hr ({unplanned_icu_readmission_72hours}/{unique_visits_without_procedures}): {unplanned_72hrs}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Metric': [\n",
    "        'All ICU Readmission', \n",
    "        'All ICU Readmission <6hr', \n",
    "        'All ICU Readmission <24hr', \n",
    "        'All ICU Readmission <48hr', \n",
    "        'All ICU Readmission <72hr', \n",
    "        'Unplanned ICU Readmission', \n",
    "        'Unplanned ICU Readmission <6hr', \n",
    "        'Unplanned ICU Readmission <24hr', \n",
    "        'Unplanned ICU Readmission <48hr', \n",
    "        'Unplanned ICU Readmission <72hr'\n",
    "    ],\n",
    "    'Numerator/Denominator': [\n",
    "        f\"{all_icu_readmission}/{total_unique_visits}\", \n",
    "        f\"{all_icu_readmission_6hours}/{total_unique_visits}\", \n",
    "        f\"{all_icu_readmission_24hours}/{total_unique_visits}\", \n",
    "        f\"{all_icu_readmission_48hours}/{total_unique_visits}\", \n",
    "        f\"{all_icu_readmission_72hours}/{total_unique_visits}\", \n",
    "        f\"{unplanned_icu_readmission}/{unique_visits_without_procedures}\", \n",
    "        f\"{unplanned_icu_readmission_6hours}/{unique_visits_without_procedures}\", \n",
    "        f\"{unplanned_icu_readmission_24hours}/{unique_visits_without_procedures}\", \n",
    "        f\"{unplanned_icu_readmission_48hours}/{unique_visits_without_procedures}\", \n",
    "        f\"{unplanned_icu_readmission_72hours}/{unique_visits_without_procedures}\"\n",
    "    ],\n",
    "    'Percentage (%)': [\n",
    "        all_icu_read, \n",
    "        all_6hrs, \n",
    "        all_24hrs, \n",
    "        all_48hrs, \n",
    "        all_72hrs, \n",
    "        unplanned_icu_read, \n",
    "        unplanned_6hrs, \n",
    "        unplanned_24hrs, \n",
    "        unplanned_48hrs, \n",
    "        unplanned_72hrs\n",
    "    ]\n",
    "}\n",
    "# Create the DataFrame\n",
    "final_df = pd.DataFrame(data)\n",
    "final_df.to_csv(f\"{output_path}ICU_readmission_rates_{datetime.today().date()}_{site_name}.csv\")\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Sankey Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_deaths_df2 = dropped_deaths_df.rename(columns={\n",
    "    'in_dttm': 'earliest_location_start',\n",
    "    'out_dttm': 'latest_location_end'\n",
    "})\n",
    "\n",
    "dropped_df_icu_discharge2 = dropped_df_icu_discharge.rename(columns={\n",
    "    'in_dttm': 'earliest_location_start',\n",
    "    'out_dttm': 'latest_location_end'\n",
    "})\n",
    "\n",
    "print(dropped_deaths_df2[\"hospitalization_id\"].nunique())\n",
    "print(dropped_df_icu_discharge2[\"hospitalization_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_df7_death = pd.concat([dropped_df_icu_discharge2[[\"patient_id\",\"hospitalization_id\",\"earliest_location_start\",\n",
    "                                                    \"latest_location_end\",\"location_category\",\"procedural\"]], \n",
    "                           dropped_deaths_df2[[\"patient_id\",\"hospitalization_id\",\"earliest_location_start\",\n",
    "                                                    \"latest_location_end\",\"location_category\",\"procedural\"]]], axis=0)\n",
    "icu_df7_death = pd.concat([icu_df7_death, icu_df7[[\"patient_id\",\"hospitalization_id\",\"earliest_location_start\",\n",
    "                                                    \"latest_location_end\",\"location_category\",\"procedural\"]]], axis=0)\n",
    "\n",
    "icu_df7_death = pd.merge(icu_df7_death,patient[[\"patient_id\",\"death_dttm\"]],on=\"patient_id\",how=\"left\")\n",
    "icu_df7_death = pd.merge(icu_df7_death,hosp[[\"hospitalization_id\",\"discharge_category\"]],on=\"hospitalization_id\",how=\"left\")\n",
    "\n",
    "\n",
    "icu_df7_death.head()\n",
    "print(\"Number of rows in df:\",len(icu_df7_death))\n",
    "print(\"Number of unique patient_id:\", icu_df7_death[\"patient_id\"].nunique())\n",
    "print(\"Number of unique hospitalization_id:\", icu_df7_death[\"hospitalization_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def mark_deaths(df):\n",
    "    # Ensure the columns are in datetime format\n",
    "    df['death_dttm'] = pd.to_datetime(df['death_dttm'], errors='coerce')\n",
    "    df['earliest_location_start'] = pd.to_datetime(df['earliest_location_start'], errors='coerce')\n",
    "    df['latest_location_end'] = pd.to_datetime(df['latest_location_end'], errors='coerce')\n",
    "\n",
    "    # Create the location_category_death column as a copy of location_category\n",
    "    df['location_category_death'] = df['location_category']\n",
    "\n",
    "    # Sort the dataframe by patient_id, hospitalization_id, and earliest_location_start to ensure order\n",
    "    df = df.sort_values(by=['hospitalization_id', 'earliest_location_start'])\n",
    "\n",
    "    # List to collect new rows that represent the death\n",
    "    new_rows = []\n",
    "\n",
    "    # Iterate over each group of hospitalization_id\n",
    "    for hospitalization_id, group in df.groupby('hospitalization_id'):\n",
    "        for idx, row in group.iterrows():\n",
    "            # Check if death_dttm is between earliest_location_start and latest_location_end\n",
    "            if (\n",
    "                not pd.isna(row['death_dttm']) and \n",
    "                row['earliest_location_start'] <= row['death_dttm'] <= row['latest_location_end']\n",
    "            ):\n",
    "                # Create a new row with the same info but marked as 'Death'\n",
    "                new_row = row.copy()\n",
    "                new_row['location_category_death'] = 'Died'\n",
    "                new_row['earliest_location_start'] = row['latest_location_end'] + pd.Timedelta(seconds=1)  # Add 1 second to ensure 'Death' comes after\n",
    "                new_rows.append(new_row)\n",
    "\n",
    "    # Add the new death rows to the original dataframe\n",
    "    df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "    # Sort the dataframe again to make sure 'Death' comes after the original location\n",
    "    df = df.sort_values(by=['hospitalization_id', 'earliest_location_start']).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Sort and apply the death marking function\n",
    "icu_df7_death = mark_deaths(icu_df7_death)\n",
    "print(\"Number of dead patients:\", icu_df7_death[icu_df7_death[\"location_category_death\"] == \"Died\"][\"hospitalization_id\"].nunique())\n",
    "print(\"Number of rows in df:\", len(icu_df7_death))\n",
    "print(\"Number of unique hospitalization_id:\", icu_df7_death[\"hospitalization_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_df7_death[icu_df7_death[\"location_category_death\"]==\"Died\"][[\"hospitalization_id\",\n",
    "                                                                  \"earliest_location_start\",\n",
    "                                                                 \"latest_location_end\",\n",
    "                                                                 \"death_dttm\",\n",
    "                                                                 \"location_category\",\n",
    "                                                                 \"location_category_death\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code chunk takes longer to run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_segments(df):\n",
    "    # Ensure data is sorted by 'earliest_location_start'\n",
    "    df = df.sort_values('earliest_location_start').reset_index(drop=True)\n",
    "\n",
    "    # Create a new column to track if the location was ICU in the original location_category\n",
    "    df['is_icu_in_location_category'] = (df['location_category'] == 'ICU')\n",
    "\n",
    "    # Detect changes in location category, both in the original and in the death-adjusted version\n",
    "    df['loc_cat_change'] = (df['location_category_death'] != df['location_category_death'].shift(1)).astype(int)\n",
    "\n",
    "    # Assign a segment_id that increments when location_category_death changes\n",
    "    df['segment_id'] = df['loc_cat_change'].cumsum()\n",
    "\n",
    "    # Group by 'segment_id' and 'location_category_death' to merge consecutive segments\n",
    "    grouped = df.groupby(['segment_id', 'location_category_death']).agg({\n",
    "        'earliest_location_start': 'min',\n",
    "        'latest_location_end': 'max',\n",
    "        'is_icu_in_location_category': 'max'  # Capture whether any part of the segment was ICU\n",
    "    }).reset_index()\n",
    "\n",
    "    # Add 'hospitalization_id' to the grouped DataFrame\n",
    "    grouped['hospitalization_id'] = df['hospitalization_id'].iloc[0]\n",
    "\n",
    "    # Rearrange columns\n",
    "    grouped = grouped[['hospitalization_id', 'segment_id', 'earliest_location_start', 'latest_location_end', 'location_category_death', 'is_icu_in_location_category']]\n",
    "    \n",
    "    # Initialize 'segment_rank' to None\n",
    "    grouped['segment_rank'] = None\n",
    "\n",
    "    # Now we can still find ICU segments based on the original location_category, but respect Death as a location in location_category_death\n",
    "    icu_segments = grouped[grouped['is_icu_in_location_category'] == 1]\n",
    "\n",
    "    if icu_segments.empty:\n",
    "        print(f\"No ICU segments for hospitalization_id: {df['hospitalization_id'].iloc[0]}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame to exclude this hospitalization\n",
    "\n",
    "    # Get the index of the first ICU segment\n",
    "    first_icu_index = icu_segments.index[0]\n",
    "\n",
    "    # Create 'segment_rank' starting from the first ICU segment\n",
    "    grouped.loc[first_icu_index:, 'segment_rank'] = range(1, len(grouped.loc[first_icu_index:]) + 1)\n",
    "\n",
    "    # Fill in the 'segment_rank' for non-ICU segments after the first ICU\n",
    "    grouped['segment_rank'] = grouped['segment_rank'].ffill()\n",
    "\n",
    "    # Compute the maximum segment rank for this hospitalization\n",
    "    max_segment_rank = grouped['segment_rank'].dropna().max()\n",
    "\n",
    "    # Add 'max_segment_rank' column to the DataFrame\n",
    "    grouped['max_segment_rank'] = max_segment_rank\n",
    "\n",
    "    # Drop the auxiliary column\n",
    "    grouped = grouped.drop(columns=['is_icu_in_location_category'])\n",
    "\n",
    "    return grouped\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "collapsed_adt = icu_df7_death[icu_df7_death[\"procedural\"] == False][['hospitalization_id', 'earliest_location_start', 'latest_location_end', 'location_category', 'location_category_death']] \\\n",
    "    .groupby('hospitalization_id', group_keys=False).apply(collapse_segments).reset_index(drop=True)\n",
    "\n",
    "print(\"Number of unique hospitalization_id:\", collapsed_adt[\"hospitalization_id\"].nunique())\n",
    "print(\"Number of rows in df:\", len(collapsed_adt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_intermediate_er2(df):\n",
    "    # Initialize an empty list to hold hospitalization_id values to be dropped\n",
    "    visits_to_drop = []\n",
    "\n",
    "    # Function to process each group of 'patient_id' and 'hospitalization_id'\n",
    "    def process_group(group):\n",
    "        # Check if 'ER' appears after the first row in the group\n",
    "        if any(group.iloc[1:]['location_category_death'] == 'ER'):\n",
    "            # Mark the visit for dropping if 'ER' appears after the first row\n",
    "            visits_to_drop.append(group.iloc[0]['hospitalization_id'])\n",
    "\n",
    "    # Apply the function to each group of'hospitalization_id'\n",
    "    df.groupby(['hospitalization_id']).apply(process_group)\n",
    "\n",
    "    # Create a new DataFrame with the rows that need to be dropped\n",
    "    dropped_df = df[df['hospitalization_id'].isin(visits_to_drop)]\n",
    "\n",
    "    # Drop the visits from the original DataFrame\n",
    "    df_cleaned = df[~df['hospitalization_id'].isin(visits_to_drop)]\n",
    "\n",
    "    return df_cleaned, dropped_df\n",
    "\n",
    "collapsed_adt = collapsed_adt.sort_values(by=['hospitalization_id', 'earliest_location_start', 'latest_location_end'])\n",
    "collapsed_adt2, dropped_df = drop_intermediate_er2(collapsed_adt)\n",
    "\n",
    "print(\"Number of unique hospitalization_id:\", collapsed_adt2[\"hospitalization_id\"].nunique())\n",
    "print(\"Number of rows in df:\", len(collapsed_adt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sankey_df = collapsed_adt2.reset_index().loc[:,['hospitalization_id','segment_rank','location_category_death']]\n",
    "sankey_df2 = sankey_df.loc[sankey_df.segment_rank.notna()] \n",
    "sankey_df3 = sankey_df2.pivot(\n",
    "    index=['hospitalization_id'],\n",
    "    columns='segment_rank',\n",
    "    values='location_category_death'\n",
    ").reset_index().fillna('Discharged')\n",
    "sankey_df4 = sankey_df3.iloc[:, :8] #only take first 7 locations, otherwise gets too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def propagate_death(df):\n",
    "    # List of columns that represent the different segments\n",
    "    segment_cols = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        # Find the first occurrence of \"Death\" in the segment columns\n",
    "        death_found = False\n",
    "        for col in segment_cols:\n",
    "            if row[col] == 'Died':\n",
    "                death_found = True\n",
    "            # If \"Death\" is found, mark all subsequent columns as \"Death\"\n",
    "            if death_found:\n",
    "                df.at[idx, col] = 'Died'\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to propagate 'Death' to subsequent columns\n",
    "sankey_df4 = propagate_death(sankey_df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = {\n",
    "    \"ICU\": 'lightcoral',\n",
    "    \"Ward\": 'skyblue',\n",
    "    'Procedural':'thistle',\n",
    "    \"Discharged\": 'lightgrey',\n",
    "    \"Died\": 'grey',\n",
    "    \"ER\":'red'\n",
    "}\n",
    "fig, ax = plt.subplots(figsize=(14, 6), constrained_layout=True)\n",
    "diag = Sankey(\n",
    "    sankey_df4.iloc[:, 1:], \n",
    "    ax=ax, \n",
    "    order=[\"ICU\", \"Ward\", \"Procedural\", \"Discharged\",\"ER\",\"Died\"],\n",
    "    block_width=0.2,\n",
    "    colors=colors,\n",
    "    alpha=0.5\n",
    ")\n",
    "diag.draw()\n",
    "ax.set_title(\"\", size=16)\n",
    "ax.set_xticks(\n",
    "    [diag.block_width / 2 + diag.flow_width * x + diag.block_width * x for x in range(sankey_df4.shape[1] - 1)]\n",
    ")\n",
    "ax.set_xticklabels(sankey_df4.columns[1:].astype(int))\n",
    "ax.set_xlabel(\"Location number\", size=14)\n",
    "ax.get_xaxis().set_visible(True)\n",
    "ax.tick_params(axis=\"x\", pad=5, labelsize=16)\n",
    "\n",
    "# Export the figure as a high-resolution image\n",
    "fig.savefig(f\"{output_path}sankey_diagram_{datetime.today().date()}_{site_name}.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Table One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unplanned_icu_final = icu_final[icu_final[\"procedural\"]==False]\n",
    "#admission location\n",
    "admission_location = icu_df7[[\"hospitalization_id\", \"location_category\"]].groupby('hospitalization_id', as_index=False).first()\n",
    "unplanned_icu_final = pd.merge(unplanned_icu_final,admission_location,on=\"hospitalization_id\",how=\"left\")\n",
    "\n",
    "#length of ICU stay \n",
    "icu_sorted = icu_df7.sort_values(by=['hospitalization_id', 'earliest_location_start'])\n",
    "icu_first_icu = icu_sorted[icu_sorted['location_category'] == 'ICU'].groupby('hospitalization_id', as_index=False).first()\n",
    "length_ICU = icu_first_icu[['hospitalization_id', 'location_hours']]\n",
    "unplanned_icu_final = pd.merge(unplanned_icu_final,length_ICU,on=\"hospitalization_id\",how=\"left\")\n",
    "unplanned_icu_final['location_days'] = unplanned_icu_final['location_hours'] / 24\n",
    "\n",
    "#in-hospital mortality\n",
    "dead_patients = collapsed_adt2[collapsed_adt2[\"location_category_death\"]==\"Died\"][[\"hospitalization_id\",\"location_category_death\"]]\n",
    "unplanned_icu_final = pd.merge(unplanned_icu_final,dead_patients,on=\"hospitalization_id\",how=\"left\")\n",
    "unplanned_icu_final['location_category_death'] = unplanned_icu_final['location_category_death'].fillna('Alive')\n",
    "\n",
    "#age at admission\n",
    "hosp = hosp.drop_duplicates(subset=[\"hospitalization_id\"])\n",
    "age = hosp[[\"hospitalization_id\",\"age_at_admission\"]]                           \n",
    "unplanned_icu_final = pd.merge(unplanned_icu_final,age,on=\"hospitalization_id\",how=\"left\")\n",
    "\n",
    "#demographics \n",
    "demog = patient[[\"patient_id\",\"race_category\",\"ethnicity_category\",\"sex_category\"]]\n",
    "unplanned_icu_final = pd.merge(unplanned_icu_final,demog,on=\"patient_id\",how=\"left\")\n",
    "unplanned_icu_final = unplanned_icu_final.drop_duplicates(subset=[\"hospitalization_id\"])\n",
    "\n",
    "#readmitted\n",
    "unplanned_icu_final['readmission'] = unplanned_icu_final['ICU_readmission_hour'].apply(\n",
    "    lambda x: 'Readmitted' if pd.notna(x) and x > 0 else 'Not Readmitted'\n",
    ")\n",
    "\n",
    "print(unplanned_icu_final[\"hospitalization_id\"].nunique())\n",
    "unplanned_icu_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_one_dat = unplanned_icu_final[[\"age_at_admission\",\n",
    "                                     \"sex_category\",\n",
    "                                     \"race_category\",\n",
    "                                    \"ethnicity_category\",\n",
    "                                    \"location_category\",\n",
    "                                     \"ICU_readmission_hour\",\n",
    "                                    \"location_days\",\n",
    "                                    \"location_category_death\",\n",
    "                                    \"readmission\"]]\n",
    "\n",
    "# Rename columns\n",
    "table_one_dat = table_one_dat.rename(columns={'age_at_admission': 'Age', \n",
    "                                              'sex_category': 'Sex', \n",
    "                                              'race_category': 'Race',\n",
    "                                             'ethnicity_category': 'Ethnicity',\n",
    "                                             'location_category': 'Initial hospital location',\n",
    "                                              'ICU_readmission_hour':'ICU readmission, hours',\n",
    "                                              \"location_days\":'ICU length of stay, days',\n",
    "                                              \"location_category_death\":'In-hospital Mortality'\n",
    "                                             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tableone import TableOne\n",
    "\n",
    "# Define the columns that include both continuous and categorical variables\n",
    "columns = [\"Age\", \"ICU readmission, hours\", \"ICU length of stay, days\", \"Sex\", \"Race\", \"Ethnicity\", \"Initial hospital location\", \"In-hospital Mortality\"]\n",
    "\n",
    "# Define which columns are categorical\n",
    "categorical = [\"Sex\", \"Race\", \"Ethnicity\", \"Initial hospital location\", \"In-hospital Mortality\"]\n",
    "\n",
    "# Define which continuous variables are not normally distributed\n",
    "nonnormal = [\"Age\",\"ICU readmission, hours\",\"ICU length of stay, days\"]\n",
    "\n",
    "#groupby\n",
    "groupby = 'readmission'\n",
    "\n",
    "# Generate the TableOne object\n",
    "table1 = TableOne(data=table_one_dat, \n",
    "                  columns=columns, \n",
    "                  categorical=categorical, \n",
    "                  nonnormal=nonnormal, \n",
    "                  groupby=groupby)\n",
    "\n",
    "# Display the table\n",
    "\n",
    "table1.to_csv(f\"{output_path}table1_{datetime.today().date()}_{site_name}.csv\")\n",
    "table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

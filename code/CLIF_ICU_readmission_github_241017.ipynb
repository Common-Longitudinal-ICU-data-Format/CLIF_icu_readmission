{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pyarrow.parquet as pq\n",
    "import time\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sankey import Sankey\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import tableone\n",
    "try:\n",
    "    from tableone import TableOne, load_dataset\n",
    "except (ModuleNotFoundError, ImportError):\n",
    "    # install on Colab\n",
    "    !pip install tableone\n",
    "    from tableone import TableOne, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from utils import config\n",
    "config = config.load_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## please change site_name, tables_path, output_path, and file_type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access configuration parameters\n",
    "site_name = config['site_name']\n",
    "tables_path = config['tables_path']\n",
    "file_type = config['file_type']\n",
    "output_path = os.path.join(\"..\", \"output\", \"final\")\n",
    "\n",
    "# Make sure the directory exists; if not, create it\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Print the configuration parameters\n",
    "print(f\"Site Name: {site_name}\")\n",
    "print(f\"Tables Path: {tables_path}\")\n",
    "print(f\"Output path: {output_path}\")\n",
    "print(f\"File Type: {file_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confirm that these are the correct paths\n",
    "adt_filepath = f\"{tables_path}/clif_adt.{file_type}\"\n",
    "hospitalization_filepath = f\"{tables_path}/clif_hospitalization.{file_type}\"\n",
    "patient_filepath = f\"{tables_path}/clif_patient.{file_type}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath, filetype):\n",
    "    start_time = time.time()  # Record the start time\n",
    "    file_name = os.path.basename(filepath) \n",
    "    if filetype == 'csv':\n",
    "        df = pd.read_csv(filepath)\n",
    "    elif filetype == 'parquet':\n",
    "        table = pq.read_table(filepath)\n",
    "        df = table.to_pandas()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please provide either 'csv' or 'parquet'.\")\n",
    "    \n",
    "    end_time = time.time()  # Record the end time\n",
    "    load_time = end_time - start_time  # Calculate the loading time\n",
    "    \n",
    "    # Calculate the size of the loaded dataset in MB\n",
    "    dataset_size_mb = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "    print(f\"File name: {file_name}\")\n",
    "    print(f\"Time taken to load the dataset: {load_time:.2f} seconds\")\n",
    "    print(f\"Size of the loaded dataset: {dataset_size_mb:.2f} MB\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "adt = read_data(adt_filepath, file_type)\n",
    "hosp = read_data(hospitalization_filepath, file_type)\n",
    "patient = read_data(patient_filepath,file_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of unique hospitalization_id:\",hosp[\"hospitalization_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_admission_dttm = hosp['admission_dttm'].isnull().sum()\n",
    "print(\"Missing admission_dttm:\", missing_admission_dttm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp2 = hosp[[\"patient_id\",\"hospitalization_id\",\"admission_dttm\",\"discharge_dttm\",\"age_at_admission\"]].copy()\n",
    "hosp2['admission_dttm'] = pd.to_datetime(hosp2['admission_dttm'])\n",
    "hosp2['discharge_dttm'] = pd.to_datetime(hosp2['discharge_dttm'])\n",
    "hosp2 = hosp2[(hosp2['admission_dttm'].dt.year >= 2020) & \n",
    "                   (hosp2['admission_dttm'].dt.year <= 2021) & \n",
    "                   (hosp2['age_at_admission'] >=18)&\n",
    "                    (hosp2['age_at_admission'] <=119)&\n",
    "                     (hosp2['hospitalization_id'].isin(adt[adt['location_category'].str.lower() == \"icu\"]['hospitalization_id'].unique()))]\n",
    "\n",
    "df = pd.merge(hosp2[[\"patient_id\",\"hospitalization_id\"]],\n",
    "                  adt[[\"hospitalization_id\",\"in_dttm\",\"out_dttm\",\"location_category\"]],\n",
    "             on=\"hospitalization_id\",how=\"left\")\n",
    "\n",
    "print(\"number of unique hospitalization_id:\",df[\"hospitalization_id\"].nunique())\n",
    "print(\"number of rows in df:\",len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"location_category\"] = df[\"location_category\"].str.lower().str.strip()\n",
    "# Mapping dictionary to update the location_category\n",
    "location_mapping = {\n",
    "    'rehab': 'Other',\n",
    "    'l&d': 'Ward',\n",
    "    'psych': 'Other',\n",
    "    'radiology': 'Other',\n",
    "    'dialysis': 'Other',\n",
    "    'hospice': 'Other',\n",
    "    'stepdown':'ICU',\n",
    "    \"icu\": \"ICU\",\n",
    "    \"er\": \"ER\",\n",
    "    \"ward\":\"Ward\",\n",
    "    \"procedural\":\"Procedural\",\n",
    "    \"other\":\"Other\",\n",
    "    'imaging':'Other',\n",
    "    'ltach':'Other'\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'location_category' column\n",
    "df['location_category'] = df['location_category'].replace(location_mapping)\n",
    "df[\"location_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sort by hospitalization_id and in_dttm\n",
    "df = df.sort_values(by=['hospitalization_id', 'in_dttm', 'out_dttm'])\n",
    "df = df[df['location_category']!= \"Other\"] #removing \"Other from the study\"\n",
    "# Apply the function to each group of hospitalization_id\n",
    "df = df.sort_values(by=['hospitalization_id', 'in_dttm', 'out_dttm'])\n",
    "#stitch together ICUs if ICU --> procedure --> ICU\n",
    "df['shifted_location'] = df['location_category'].shift(1)\n",
    "df['new_location'] = (df['location_category'] != df['shifted_location']).cumsum()\n",
    "df = df.groupby(['hospitalization_id','new_location']).agg(\n",
    "    in_dttm=pd.NamedAgg(column='in_dttm', aggfunc='min'),\n",
    "    out_dttm=pd.NamedAgg(column='out_dttm', aggfunc='max'),\n",
    "    location_category=pd.NamedAgg(column='location_category', aggfunc='first'), \n",
    "    patient_id=pd.NamedAgg(column='patient_id', aggfunc='first')\n",
    ").reset_index()\n",
    "\n",
    "# Assuming 'df' is your DataFrame and it's sorted appropriately\n",
    "df.sort_values(['hospitalization_id', 'in_dttm'], inplace=True)  # Replace 'timestamp_column' with the actual column name that orders your events chronologically.\n",
    "# Use groupby and shift to get previous and next 'location_category' within each hospitalization_id\n",
    "df['prev_category'] = df.groupby('hospitalization_id')['location_category'].shift(1)\n",
    "df['next_category'] = df.groupby('hospitalization_id')['location_category'].shift(-1)\n",
    "# Create a boolean mask for 'Procedural' rows sandwiched between 'ICU' rows\n",
    "mask = (\n",
    "    (df['location_category'] == 'Procedural') &\n",
    "    (df['prev_category'] == 'ICU') &\n",
    "    (df['next_category'] == 'ICU')\n",
    ")\n",
    "# Remove the rows where the mask is True\n",
    "df = df[~mask].reset_index(drop=True)\n",
    "# Drop the helper columns\n",
    "df.drop(columns=['prev_category', 'next_category'], inplace=True)\n",
    "\n",
    "print(\"Number of unique hospitalization_id:\", df[\"hospitalization_id\"].nunique())\n",
    "print(\"Number of rows in df:\",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_stays(df):\n",
    "    # Ensure the in_dttm and out_dttm columns are in datetime format\n",
    "    df['in_dttm'] = pd.to_datetime(df['in_dttm'], errors='coerce')\n",
    "    df['out_dttm'] = pd.to_datetime(df['out_dttm'], errors='coerce')\n",
    "\n",
    "    # Directly filter rows based on the time difference without creating a new column\n",
    "    mask = (df['out_dttm'] - df['in_dttm']) >= pd.Timedelta(hours=1)\n",
    "    df_filtered = df[mask]\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# Sort the DataFrame once\n",
    "df = df.sort_values(by=['hospitalization_id', 'in_dttm', 'out_dttm'])\n",
    "\n",
    "# Remove short stays\n",
    "df = remove_short_stays(df)\n",
    "\n",
    "# Efficiently shift and calculate new locations\n",
    "df['shifted_location'] = df['location_category'].shift(1)\n",
    "df['new_location'] = (df['location_category'] != df['shifted_location']).cumsum()\n",
    "\n",
    "# Group by hospitalization_id and new_location in one operation, minimize memory use\n",
    "df = df.groupby(['hospitalization_id', 'new_location'], as_index=False).agg(\n",
    "    in_dttm=('in_dttm', 'min'),\n",
    "    out_dttm=('out_dttm', 'max'),\n",
    "    location_category=('location_category', 'first'),\n",
    "    patient_id=('patient_id', 'first')\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(\"Number of unique hospitalization_id:\", df[\"hospitalization_id\"].nunique())\n",
    "print(\"Number of rows in df:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert in_dttm and out_dttm columns to datetime format\n",
    "df['in_dttm'] = pd.to_datetime(df['in_dttm'])\n",
    "df['out_dttm'] = pd.to_datetime(df['out_dttm'])\n",
    "\n",
    "# Sort the DataFrame\n",
    "df = df.sort_values(by=['hospitalization_id', 'in_dttm', 'out_dttm'])\n",
    "\n",
    "# Function to mark rows as procedural if 'Procedural' is immediately before the second ICU\n",
    "def mark_procedural_before_second_icu(df):\n",
    "    # Find ICU rows\n",
    "    df['is_icu'] = df['location_category'] == 'ICU'\n",
    "    \n",
    "    # Create a group-level cumulative sum to count ICU admissions\n",
    "    df['icu_count'] = df.groupby(['patient_id', 'hospitalization_id'])['is_icu'].cumsum()\n",
    "\n",
    "    # Create a 'procedural' column that is initially False\n",
    "    df['procedural'] = False\n",
    "\n",
    "    # Identify rows where the second ICU occurs\n",
    "    for hosp_id, group in df.groupby(['patient_id', 'hospitalization_id']):\n",
    "        # Find the indices of ICU rows\n",
    "        icu_rows = group[group['location_category'] == 'ICU'].index\n",
    "\n",
    "        # If there are at least two ICU admissions\n",
    "        if len(icu_rows) > 1:\n",
    "            second_icu_index = icu_rows[1]  # Second ICU occurrence\n",
    "            \n",
    "            # Check the row immediately before the second ICU\n",
    "            if second_icu_index - 1 in group.index and group.loc[second_icu_index - 1, 'location_category'] == 'Procedural':\n",
    "                # Mark the row as procedural\n",
    "                df.loc[second_icu_index - 1, 'procedural'] = True\n",
    "                \n",
    "    # Propagate procedural == True to all rows in the same hospitalization_id if any row is marked as True\n",
    "    df['procedural'] = df.groupby('hospitalization_id')['procedural'].transform('max')\n",
    "\n",
    "    # Drop helper columns\n",
    "    df.drop(columns=['is_icu', 'icu_count'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Further transformations as needed\n",
    "df2 = df.copy()\n",
    "\n",
    "# Ensure dataset is aggregated correctly\n",
    "df2['shifted_location'] = df2['location_category'].shift(1)\n",
    "df2['new_location'] = (df2['location_category'] != df2['shifted_location']).cumsum()\n",
    "\n",
    "df2 = df2.groupby(['hospitalization_id', 'new_location']).agg(\n",
    "    in_dttm=('in_dttm', 'min'),\n",
    "    out_dttm=('out_dttm', 'max'),\n",
    "    location_category=('location_category', 'first'),\n",
    "    patient_id=('patient_id', 'first')\n",
    ").reset_index()\n",
    "\n",
    "# Apply the function to mark procedural rows\n",
    "df2 = mark_procedural_before_second_icu(df2)\n",
    "\n",
    "# Output the updated DataFrame\n",
    "print(\"Number of unique hospitalization_id:\", df2[\"hospitalization_id\"].nunique())\n",
    "print(\"Number of rows in df:\", len(df2))\n",
    "print(\"Number of hospitalizations marked as procedural:\", df2[df2[\"procedural\"] == True][\"hospitalization_id\"].nunique())\n",
    "df2[['patient_id', 'hospitalization_id', 'location_category', 'procedural']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'in_dttm' and 'out_dttm' columns to datetime format (vectorized)\n",
    "df2['in_dttm'] = pd.to_datetime(df2['in_dttm'])\n",
    "df2['out_dttm'] = pd.to_datetime(df2['out_dttm'])\n",
    "\n",
    "# Sort the DataFrame by hospitalization_id and datetime columns\n",
    "df2 = df2.sort_values(by=['hospitalization_id', 'in_dttm', 'out_dttm'])\n",
    "\n",
    "# Filter directly for ICU visits\n",
    "icu_visits = df2[df2['location_category'] == \"ICU\"]\n",
    "\n",
    "# Get the unique hospitalization_ids that had an ICU visit\n",
    "icu_visit_ids = icu_visits['hospitalization_id'].unique()\n",
    "\n",
    "# Filter the entire DataFrame to include only rows with those hospitalization_ids\n",
    "icu_df = df2[df2['hospitalization_id'].isin(icu_visit_ids)]\n",
    "\n",
    "# Sort the filtered DataFrame again if needed\n",
    "icu_df = icu_df.sort_values(by=['hospitalization_id', 'in_dttm', 'out_dttm'])\n",
    "\n",
    "# Output the resulting DataFrame and summary statistics\n",
    "print(\"Number of unique hospitalization_id:\", icu_df[\"hospitalization_id\"].nunique())\n",
    "print(\"Number of rows in df:\", len(icu_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_deaths_during_icu(df, patient_df):\n",
    "    # Merge death_dttm from patient data into icu_df, only keep relevant columns in patient_df\n",
    "    df = df.merge(patient_df[['patient_id', 'death_dttm']], on='patient_id', how='left')\n",
    "\n",
    "    # Convert 'death_dttm', 'in_dttm', and 'out_dttm' to datetime format (more efficient bulk conversion)\n",
    "    df[['death_dttm', 'in_dttm', 'out_dttm']] = df[['death_dttm', 'in_dttm', 'out_dttm']].apply(pd.to_datetime, errors='coerce')\n",
    "\n",
    "    # Filter directly for the first ICU admission per hospitalization_id\n",
    "    icu_first_admission = df[df['location_category'] == 'ICU'].sort_values('in_dttm').drop_duplicates('hospitalization_id')\n",
    "\n",
    "    # Identify hospitalization_id where the patient died during ICU admission\n",
    "    death_during_icu = icu_first_admission.loc[\n",
    "        (icu_first_admission['death_dttm'].notnull()) &\n",
    "        (icu_first_admission['death_dttm'] >= icu_first_admission['in_dttm']) &\n",
    "        (icu_first_admission['death_dttm'] <= icu_first_admission['out_dttm']),\n",
    "        'hospitalization_id'\n",
    "    ]\n",
    "\n",
    "    # Filter out rows where the patient died during ICU admission\n",
    "    df_cleaned = df[~df['hospitalization_id'].isin(death_during_icu)]\n",
    "    dropped_df = df[df['hospitalization_id'].isin(death_during_icu)]\n",
    "\n",
    "    return df_cleaned, dropped_df\n",
    "\n",
    "# Sorting the icu_df by 'hospitalization_id' and 'in_dttm'\n",
    "icu_df = icu_df.sort_values(by=['hospitalization_id', 'in_dttm'])\n",
    "\n",
    "# Apply the function\n",
    "icu_df2, dropped_deaths_df = drop_deaths_during_icu(icu_df, patient)\n",
    "\n",
    "# Output statistics\n",
    "print(\"Number of unique hospitalization_id with death during ICU:\", dropped_deaths_df[\"hospitalization_id\"].nunique())\n",
    "print(\"Number of unique hospitalization_id after removing death during ICU:\", icu_df2[\"hospitalization_id\"].nunique())\n",
    "print(\"Number of rows in df:\", len(icu_df2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_direct_icu_discharges(df):\n",
    "    # Sort the DataFrame by hospitalization_id and in_dttm if not already sorted\n",
    "    df = df.sort_values(by=['hospitalization_id', 'in_dttm'])\n",
    "\n",
    "    # Identify the first ICU admission for each hospitalization_id\n",
    "    first_icu = df[df['location_category'] == 'ICU'].groupby('hospitalization_id').head(1)\n",
    "\n",
    "    # Identify the last row for each hospitalization_id\n",
    "    last_row = df.groupby('hospitalization_id').tail(1)\n",
    "\n",
    "    # Correct the logic: Check where the first ICU row is also the last row in the hospitalization\n",
    "    visits_to_drop = first_icu[first_icu.index.isin(last_row.index)]['hospitalization_id']\n",
    "\n",
    "    # Filter out the rows for hospitalizations where patients were discharged directly from ICU\n",
    "    df_cleaned = df[~df['hospitalization_id'].isin(visits_to_drop)]\n",
    "    dropped_df = df[df['hospitalization_id'].isin(visits_to_drop)]\n",
    "\n",
    "    # Drop helper columns (if any)\n",
    "    df_cleaned.drop(columns=['is_icu'], inplace=True, errors='ignore')\n",
    "\n",
    "    return df_cleaned, dropped_df\n",
    "\n",
    "# Sort the input DataFrame\n",
    "icu_df2 = icu_df2.sort_values(by=['hospitalization_id', 'in_dttm', 'out_dttm'])\n",
    "\n",
    "# Apply the optimized function\n",
    "icu_df3, dropped_df_icu_discharge = drop_direct_icu_discharges(icu_df2)\n",
    "\n",
    "# Output the counts and dropped DataFrame\n",
    "print(\"Number of unique hospitalization_id after filtering:\", icu_df3[\"hospitalization_id\"].nunique())\n",
    "print(\"Number of rows in df:\", len(icu_df3))\n",
    "print(\"Number of unique patient_id dropped:\", dropped_df_icu_discharge[\"patient_id\"].nunique())\n",
    "print(\"Number of unique hospitalization_id dropped:\", dropped_df_icu_discharge[\"hospitalization_id\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redundant but making sure that the location_category is grouped together \n",
    "icu_df4 = icu_df3.sort_values(by=['hospitalization_id', 'in_dttm','out_dttm'])\n",
    "icu_df4['shifted_location'] = icu_df4['location_category'].shift(1)\n",
    "icu_df4['new_location'] = (icu_df4['location_category'] != icu_df4['shifted_location']).cumsum()\n",
    "\n",
    "icu_df4 = icu_df4.groupby(['hospitalization_id','new_location']).agg(\n",
    "    earliest_location_start=pd.NamedAgg(column='in_dttm', aggfunc='min'),\n",
    "    latest_location_end=pd.NamedAgg(column='out_dttm', aggfunc='max'),\n",
    "    location_category=pd.NamedAgg(column='location_category', aggfunc='first'), \n",
    "    patient_id=pd.NamedAgg(column='patient_id', aggfunc='first'),\n",
    "    procedural = pd.NamedAgg(column='procedural', aggfunc='first')\n",
    ").reset_index()\n",
    "\n",
    "print(\"Number of unique hospitalization_id:\", icu_df4[\"hospitalization_id\"].nunique())\n",
    "icu_df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_intermediate_er(df):\n",
    "    # Sort the DataFrame to ensure it's ordered correctly by hospitalization_id and location timing\n",
    "    df = df.sort_values(by=['hospitalization_id', 'earliest_location_start', 'latest_location_end'])\n",
    "\n",
    "    # Create a mask for rows that are the first row within each hospitalization_id group\n",
    "    first_row_mask = df.groupby('hospitalization_id').cumcount() == 0\n",
    "\n",
    "    # Create a mask for rows where the location_category is 'ER'\n",
    "    er_mask = df['location_category'] == 'ER'\n",
    "\n",
    "    # Find the hospitalization_ids where 'ER' occurs after the first row\n",
    "    er_after_first_row = df[~first_row_mask & er_mask]['hospitalization_id'].unique()\n",
    "\n",
    "    # Filter the DataFrame to drop those hospitalization_ids\n",
    "    df_cleaned = df[~df['hospitalization_id'].isin(er_after_first_row)]\n",
    "    dropped_df = df[df['hospitalization_id'].isin(er_after_first_row)]\n",
    "\n",
    "    return df_cleaned, dropped_df\n",
    "\n",
    "# Sort and apply the drop function\n",
    "icu_df4 = icu_df4.sort_values(by=['hospitalization_id', 'earliest_location_start', 'latest_location_end'])\n",
    "icu_df5, dropped_df = drop_intermediate_er(icu_df4)\n",
    "\n",
    "# Convert to datetime and calculate location hours\n",
    "icu_df5['earliest_location_start'] = pd.to_datetime(icu_df5['earliest_location_start'])\n",
    "icu_df5['latest_location_end'] = pd.to_datetime(icu_df5['latest_location_end'])\n",
    "icu_df5['location_hours'] = (icu_df5['latest_location_end'] - icu_df5['earliest_location_start']).dt.total_seconds() / 3600\n",
    "\n",
    "# Output the results\n",
    "print(\"Number of rows in df:\", len(icu_df5))\n",
    "print(\"Number of unique hospitalization_id:\", icu_df5[\"hospitalization_id\"].nunique())\n",
    "icu_df5.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICU readmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_icu_readmission_time(df):\n",
    "    # Initialize a new column 'ICU_readmission_hour' with None\n",
    "    df['ICU_readmission_hour'] = None\n",
    "\n",
    "    # Function to process each group\n",
    "    def process_group(group):\n",
    "        # Find the rows where location_category is ICU\n",
    "        icu_rows = group[group['location_category'] == 'ICU']\n",
    "\n",
    "        # Check if there are at least two ICU events\n",
    "        if len(icu_rows) > 1:\n",
    "            # Sort by earliest_start2 to ensure correct order\n",
    "            icu_rows = icu_rows.sort_values(by='earliest_location_start')\n",
    "\n",
    "            # Get the first and second ICU event\n",
    "            first_icu = icu_rows.iloc[0]\n",
    "            second_icu = icu_rows.iloc[1]\n",
    "\n",
    "            # Calculate the time difference between first and second ICU events in hours\n",
    "            time_diff = (second_icu['earliest_location_start'] - first_icu['latest_location_end']).total_seconds() / 3600\n",
    "\n",
    "            # Update the entire group with the calculated time difference\n",
    "            df.loc[group.index, 'ICU_readmission_hour'] = time_diff\n",
    "\n",
    "    # Apply the function to each group of 'person_id' and 'custom_visit_occurrence2'\n",
    "    df.groupby(['patient_id', 'hospitalization_id']).apply(process_group)\n",
    "\n",
    "    return df\n",
    "\n",
    "icu_df6 = calculate_icu_readmission_time(icu_df5)\n",
    "print(\"number of rows in df:\",len(icu_df6))\n",
    "print(\"Number of unique hospitalization_id:\", icu_df6[\"hospitalization_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify hospitalization_ids where ICU_readmission_hour is less than 0\n",
    "#This is in place incase there are errors in the adt, should be a very small number if any \n",
    "hosp_ids_to_drop = icu_df6[icu_df6[\"ICU_readmission_hour\"] <=0][\"hospitalization_id\"].unique()\n",
    "\n",
    "# Drop rows with those hospitalization_ids\n",
    "icu_df7 = icu_df6[~icu_df6[\"hospitalization_id\"].isin(hosp_ids_to_drop)]\n",
    "\n",
    "# Check the result\n",
    "print(\"Number of unique hospitalization_id after filtering:\", icu_df7[\"hospitalization_id\"].nunique())\n",
    "print(\"Number of rows in df after filtering:\", len(icu_df7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#icu_df6[icu_df6[\"ICU_readmission_hour\"]>0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_df7[icu_df7[\"procedural\"]==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns\n",
    "icu_final = icu_df7[[\"patient_id\", \"hospitalization_id\", \"ICU_readmission_hour\",\"procedural\"]]\n",
    "\n",
    "# Drop duplicates based on 'custom_visit_occurrence2'\n",
    "icu_final = icu_final.drop_duplicates(subset=[\"hospitalization_id\"])\n",
    "\n",
    "# Display the first few rows of the result\n",
    "print(\"Number of rows in df:\",len(icu_final))\n",
    "print(\"Number of unique patient_id:\", icu_final[\"patient_id\"].nunique())\n",
    "print(\"Number of unique hospitalization_id:\", icu_final[\"hospitalization_id\"].nunique())\n",
    "icu_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ICU Readmission Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_unique_visits = icu_final[\"hospitalization_id\"].nunique()\n",
    "unique_visits_with_procedures = icu_final[icu_final[\"procedural\"]==True][\"hospitalization_id\"].nunique()\n",
    "unique_visits_without_procedures = icu_final[icu_final[\"procedural\"]==False][\"hospitalization_id\"].nunique()\n",
    "all_icu_readmission = icu_final[(icu_final[\"ICU_readmission_hour\"]>0)][\"hospitalization_id\"].nunique()\n",
    "all_icu_readmission_6hours = icu_final[(icu_final[\"ICU_readmission_hour\"]<6)][\"hospitalization_id\"].nunique()\n",
    "all_icu_readmission_24hours = icu_final[(icu_final[\"ICU_readmission_hour\"]<24)][\"hospitalization_id\"].nunique()\n",
    "all_icu_readmission_48hours = icu_final[(icu_final[\"ICU_readmission_hour\"]<48)][\"hospitalization_id\"].nunique()\n",
    "all_icu_readmission_72hours = icu_final[(icu_final[\"ICU_readmission_hour\"]<72)][\"hospitalization_id\"].nunique()\n",
    "unplanned_icu_readmission = icu_final[(icu_final[\"procedural\"]==False)&(icu_final[\"ICU_readmission_hour\"]>0)][\"hospitalization_id\"].nunique()\n",
    "unplanned_icu_readmission_6hours = icu_final[(icu_final[\"procedural\"]==False)&(icu_final[\"ICU_readmission_hour\"]<6)][\"hospitalization_id\"].nunique()\n",
    "unplanned_icu_readmission_24hours = icu_final[(icu_final[\"procedural\"]==False)&(icu_final[\"ICU_readmission_hour\"]<24)][\"hospitalization_id\"].nunique()\n",
    "unplanned_icu_readmission_48hours = icu_final[(icu_final[\"procedural\"]==False)&(icu_final[\"ICU_readmission_hour\"]<48)][\"hospitalization_id\"].nunique()\n",
    "unplanned_icu_readmission_72hours = icu_final[(icu_final[\"procedural\"]==False)&(icu_final[\"ICU_readmission_hour\"]<72)][\"hospitalization_id\"].nunique()\n",
    "\n",
    "\n",
    "print(\"Number of unique hospitalization_id:\", total_unique_visits)\n",
    "print(\"Planned readmission: Number of unique hospitalization_id with procedures between ICU readmission:\", unique_visits_with_procedures)\n",
    "print(\"Unplanned readmission: Number of unique hospitalization_id without any procedures between ICU readmission:\", unique_visits_without_procedures)\n",
    "\n",
    "all_icu_read = round((all_icu_readmission / total_unique_visits)*100,2)\n",
    "all_6hrs = round((all_icu_readmission_6hours/total_unique_visits)*100,2)\n",
    "all_24hrs = round((all_icu_readmission_24hours/total_unique_visits)*100,2)\n",
    "all_48hrs = round((all_icu_readmission_48hours/total_unique_visits)*100,2)\n",
    "all_72hrs = round((all_icu_readmission_72hours/total_unique_visits)*100,2)\n",
    "\n",
    "unplanned_icu_read = round((unplanned_icu_readmission / unique_visits_without_procedures)*100,2)\n",
    "unplanned_6hrs = round((unplanned_icu_readmission_6hours/unique_visits_without_procedures)*100,2)\n",
    "unplanned_24hrs = round((unplanned_icu_readmission_24hours/unique_visits_without_procedures)*100,2)\n",
    "unplanned_48hrs = round((unplanned_icu_readmission_48hours/unique_visits_without_procedures)*100,2)\n",
    "unplanned_72hrs = round((unplanned_icu_readmission_72hours/unique_visits_without_procedures)*100,2)\n",
    "\n",
    "# Print all readmission rate\n",
    "print(f\"All ICU readmission ({all_icu_readmission}/{total_unique_visits}): {all_icu_read}%\")\n",
    "print(f\"All ICU readmission <6hr ({all_icu_readmission_6hours}/{total_unique_visits}): {all_6hrs}%\")\n",
    "print(f\"All ICU readmission <24hr ({all_icu_readmission_24hours}/{total_unique_visits}): {all_24hrs}%\")\n",
    "print(f\"All ICU readmission <48hr ({all_icu_readmission_48hours}/{total_unique_visits}): {all_48hrs}%\")\n",
    "print(f\"All ICU readmission <72hr ({all_icu_readmission_72hours}/{total_unique_visits}): {all_72hrs}%\")\n",
    "\n",
    "# Print unplanned readmission rate\n",
    "print(f\"Unplanned ICU readmission ({unplanned_icu_readmission}/{unique_visits_without_procedures}): {unplanned_icu_read}%\")\n",
    "print(f\"Unplanned ICU readmission <6hr ({unplanned_icu_readmission_6hours}/{unique_visits_without_procedures}): {unplanned_6hrs}%\")\n",
    "print(f\"Unplanned ICU readmission <24hr ({unplanned_icu_readmission_24hours}/{unique_visits_without_procedures}): {unplanned_24hrs}%\")\n",
    "print(f\"Unplanned ICU readmission <48hr ({unplanned_icu_readmission_48hours}/{unique_visits_without_procedures}): {unplanned_48hrs}%\")\n",
    "print(f\"Unplanned ICU readmission <72hr ({unplanned_icu_readmission_72hours}/{unique_visits_without_procedures}): {unplanned_72hrs}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Metric': [\n",
    "        'All ICU Readmission', \n",
    "        'All ICU Readmission <6hr', \n",
    "        'All ICU Readmission <24hr', \n",
    "        'All ICU Readmission <48hr', \n",
    "        'All ICU Readmission <72hr', \n",
    "        'Unplanned ICU Readmission', \n",
    "        'Unplanned ICU Readmission <6hr', \n",
    "        'Unplanned ICU Readmission <24hr', \n",
    "        'Unplanned ICU Readmission <48hr', \n",
    "        'Unplanned ICU Readmission <72hr'\n",
    "    ],\n",
    "    'Numerator/Denominator': [\n",
    "        f\"{all_icu_readmission}/{total_unique_visits}\", \n",
    "        f\"{all_icu_readmission_6hours}/{total_unique_visits}\", \n",
    "        f\"{all_icu_readmission_24hours}/{total_unique_visits}\", \n",
    "        f\"{all_icu_readmission_48hours}/{total_unique_visits}\", \n",
    "        f\"{all_icu_readmission_72hours}/{total_unique_visits}\", \n",
    "        f\"{unplanned_icu_readmission}/{unique_visits_without_procedures}\", \n",
    "        f\"{unplanned_icu_readmission_6hours}/{unique_visits_without_procedures}\", \n",
    "        f\"{unplanned_icu_readmission_24hours}/{unique_visits_without_procedures}\", \n",
    "        f\"{unplanned_icu_readmission_48hours}/{unique_visits_without_procedures}\", \n",
    "        f\"{unplanned_icu_readmission_72hours}/{unique_visits_without_procedures}\"\n",
    "    ],\n",
    "    'Percentage (%)': [\n",
    "        all_icu_read, \n",
    "        all_6hrs, \n",
    "        all_24hrs, \n",
    "        all_48hrs, \n",
    "        all_72hrs, \n",
    "        unplanned_icu_read, \n",
    "        unplanned_6hrs, \n",
    "        unplanned_24hrs, \n",
    "        unplanned_48hrs, \n",
    "        unplanned_72hrs\n",
    "    ]\n",
    "}\n",
    "# Create the DataFrame\n",
    "final_df = pd.DataFrame(data)\n",
    "final_df.to_csv(f'{output_path}/ICU_readmission_rates_{site_name}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv')\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Sankey Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_deaths_df2 = dropped_deaths_df.rename(columns={\n",
    "    'in_dttm': 'earliest_location_start',\n",
    "    'out_dttm': 'latest_location_end'\n",
    "})\n",
    "\n",
    "dropped_df_icu_discharge2 = dropped_df_icu_discharge.rename(columns={\n",
    "    'in_dttm': 'earliest_location_start',\n",
    "    'out_dttm': 'latest_location_end'\n",
    "})\n",
    "\n",
    "print(dropped_deaths_df2[\"hospitalization_id\"].nunique())\n",
    "print(dropped_df_icu_discharge2[\"hospitalization_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_df7_death = pd.concat([dropped_df_icu_discharge2[[\"patient_id\",\"hospitalization_id\",\"earliest_location_start\",\n",
    "                                                    \"latest_location_end\",\"location_category\",\"procedural\"]], \n",
    "                           dropped_deaths_df2[[\"patient_id\",\"hospitalization_id\",\"earliest_location_start\",\n",
    "                                                    \"latest_location_end\",\"location_category\",\"procedural\"]]], axis=0)\n",
    "icu_df7_death = pd.concat([icu_df7_death, icu_df7[[\"patient_id\",\"hospitalization_id\",\"earliest_location_start\",\n",
    "                                                    \"latest_location_end\",\"location_category\",\"procedural\"]]], axis=0)\n",
    "\n",
    "icu_df7_death = pd.merge(icu_df7_death,patient[[\"patient_id\",\"death_dttm\"]],on=\"patient_id\",how=\"left\")\n",
    "icu_df7_death = pd.merge(icu_df7_death,hosp[[\"hospitalization_id\",\"discharge_category\"]],on=\"hospitalization_id\",how=\"left\")\n",
    "\n",
    "\n",
    "icu_df7_death.head()\n",
    "print(\"Number of rows in df:\",len(icu_df7_death))\n",
    "print(\"Number of unique patient_id:\", icu_df7_death[\"patient_id\"].nunique())\n",
    "print(\"Number of unique hospitalization_id:\", icu_df7_death[\"hospitalization_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_deaths(df):\n",
    "    # Ensure the columns are in datetime format\n",
    "    df['death_dttm'] = pd.to_datetime(df['death_dttm'], errors='coerce').dt.tz_localize(None)\n",
    "    df['earliest_location_start'] = pd.to_datetime(df['earliest_location_start'], errors='coerce').dt.tz_localize(None)\n",
    "    df['latest_location_end'] = pd.to_datetime(df['latest_location_end'], errors='coerce').dt.tz_localize(None)\n",
    "\n",
    "    # Create the location_category_death column as a copy of location_category\n",
    "    df['location_category_death'] = df['location_category']\n",
    "\n",
    "    # Sort the dataframe by hospitalization_id and earliest_location_start to ensure order\n",
    "    df = df.sort_values(by=['hospitalization_id', 'earliest_location_start'])\n",
    "\n",
    "    # Vectorized condition to check if death_dttm is between earliest_location_start and latest_location_end\n",
    "    death_condition = (\n",
    "        df['death_dttm'].notna() & \n",
    "        (df['earliest_location_start'] <= df['death_dttm']) & \n",
    "        (df['death_dttm'] <= df['latest_location_end'])\n",
    "    )\n",
    "\n",
    "    # Get unique hospitalization_ids where death occurs during the stay\n",
    "    death_hosp_ids = df.loc[death_condition, 'hospitalization_id'].unique()\n",
    "\n",
    "    # Get the last row of each hospitalization where death occurs\n",
    "    death_rows = df[df['hospitalization_id'].isin(death_hosp_ids)].groupby('hospitalization_id').last().reset_index()\n",
    "\n",
    "    # Modify those rows to represent the 'Death' event\n",
    "    death_rows['location_category_death'] = 'Died'\n",
    "    death_rows['earliest_location_start'] = death_rows['latest_location_end'] + pd.Timedelta(seconds=1)\n",
    "    death_rows['latest_location_end'] = death_rows['earliest_location_start']  # Ensure 'Death' row has valid time window\n",
    "\n",
    "    # Concatenate the new 'Death' rows to the original dataframe\n",
    "    df = pd.concat([df, death_rows], ignore_index=True)\n",
    "\n",
    "    # Sort the dataframe again to ensure 'Death' rows come after the original locations\n",
    "    df = df.sort_values(by=['hospitalization_id', 'earliest_location_start']).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "# Apply the function to the DataFrame\n",
    "icu_df7_death = mark_deaths(icu_df7_death)\n",
    "\n",
    "# Output the results\n",
    "print(\"Number of dead patients:\", icu_df7_death[icu_df7_death[\"location_category_death\"] == \"Died\"][\"hospitalization_id\"].nunique())\n",
    "print(\"Number of rows in df:\", len(icu_df7_death))\n",
    "print(\"Number of unique hospitalization_id:\", icu_df7_death[\"hospitalization_id\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_df7_death[icu_df7_death[\"location_category_death\"]==\"Died\"][[\"hospitalization_id\",\n",
    "                                                                  \"earliest_location_start\",\n",
    "                                                                 \"latest_location_end\",\n",
    "                                                                 \"death_dttm\",\n",
    "                                                                 \"location_category\",\n",
    "                                                                 \"location_category_death\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code chunk takes longer to run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_segments(df):\n",
    "    # Ensure data is sorted by 'earliest_location_start'\n",
    "    df = df.sort_values('earliest_location_start').reset_index(drop=True)\n",
    "\n",
    "    # Create a new column to track if the location was ICU in the original location_category\n",
    "    df['is_icu_in_location_category'] = (df['location_category'] == 'ICU')\n",
    "\n",
    "    # Detect changes in location category, both in the original and in the death-adjusted version\n",
    "    df['loc_cat_change'] = (df['location_category_death'] != df['location_category_death'].shift(1)).astype(int)\n",
    "\n",
    "    # Assign a segment_id that increments when location_category_death changes\n",
    "    df['segment_id'] = df['loc_cat_change'].cumsum()\n",
    "\n",
    "    # Group by 'segment_id' and 'location_category_death' to merge consecutive segments\n",
    "    grouped = df.groupby(['segment_id', 'location_category_death']).agg({\n",
    "        'earliest_location_start': 'min',\n",
    "        'latest_location_end': 'max',\n",
    "        'is_icu_in_location_category': 'max'  # Capture whether any part of the segment was ICU\n",
    "    }).reset_index()\n",
    "\n",
    "    # Add 'hospitalization_id' to the grouped DataFrame\n",
    "    grouped['hospitalization_id'] = df['hospitalization_id'].iloc[0]\n",
    "\n",
    "    # Rearrange columns\n",
    "    grouped = grouped[['hospitalization_id', 'segment_id', 'earliest_location_start', 'latest_location_end', 'location_category_death', 'is_icu_in_location_category']]\n",
    "    \n",
    "    # Initialize 'segment_rank' to None\n",
    "    grouped['segment_rank'] = None\n",
    "\n",
    "    # Now we can still find ICU segments based on the original location_category, but respect Death as a location in location_category_death\n",
    "    icu_segments = grouped[grouped['is_icu_in_location_category'] == 1]\n",
    "\n",
    "    if icu_segments.empty:\n",
    "        print(f\"No ICU segments for hospitalization_id: {df['hospitalization_id'].iloc[0]}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame to exclude this hospitalization\n",
    "\n",
    "    # Get the index of the first ICU segment\n",
    "    first_icu_index = icu_segments.index[0]\n",
    "\n",
    "    # Create 'segment_rank' starting from the first ICU segment\n",
    "    grouped.loc[first_icu_index:, 'segment_rank'] = range(1, len(grouped.loc[first_icu_index:]) + 1)\n",
    "\n",
    "    # Fill in the 'segment_rank' for non-ICU segments after the first ICU\n",
    "    grouped['segment_rank'] = grouped['segment_rank'].ffill()\n",
    "\n",
    "    # Compute the maximum segment rank for this hospitalization\n",
    "    max_segment_rank = grouped['segment_rank'].dropna().max()\n",
    "\n",
    "    # Add 'max_segment_rank' column to the DataFrame\n",
    "    grouped['max_segment_rank'] = max_segment_rank\n",
    "\n",
    "    # Drop the auxiliary column\n",
    "    grouped = grouped.drop(columns=['is_icu_in_location_category'])\n",
    "\n",
    "    return grouped\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "collapsed_adt = icu_df7_death[icu_df7_death[\"procedural\"] == False][['hospitalization_id', 'earliest_location_start', 'latest_location_end', 'location_category', 'location_category_death']] \\\n",
    "    .groupby('hospitalization_id', group_keys=False).apply(collapse_segments).reset_index(drop=True)\n",
    "\n",
    "print(\"Number of unique hospitalization_id:\", collapsed_adt[\"hospitalization_id\"].nunique())\n",
    "print(\"Number of rows in df:\", len(collapsed_adt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_intermediate_er2(df):\n",
    "    # Sort the DataFrame to ensure correct order of rows within each hospitalization_id\n",
    "    df = df.sort_values(by=['hospitalization_id', 'earliest_location_start', 'latest_location_end'])\n",
    "\n",
    "    # Create a boolean mask to detect if 'ER' appears after the first row in each hospitalization\n",
    "    df['is_first_row'] = df.groupby('hospitalization_id').cumcount() == 0\n",
    "\n",
    "    # Group by 'hospitalization_id' and check if any 'ER' appears after the first row\n",
    "    er_after_first_row = df.groupby('hospitalization_id').apply(\n",
    "        lambda group: (group.loc[~group['is_first_row'], 'location_category_death'] == 'ER').any()\n",
    "    )\n",
    "\n",
    "    # Find hospitalization_ids where 'ER' appears after the first row\n",
    "    visits_to_drop = er_after_first_row[er_after_first_row].index\n",
    "\n",
    "    # Drop rows from the original DataFrame where hospitalization_id is in visits_to_drop\n",
    "    df_cleaned = df[~df['hospitalization_id'].isin(visits_to_drop)]\n",
    "    dropped_df = df[df['hospitalization_id'].isin(visits_to_drop)]\n",
    "\n",
    "    # Drop the 'is_first_row' helper column\n",
    "    df_cleaned = df_cleaned.drop(columns=['is_first_row'])\n",
    "\n",
    "    return df_cleaned, dropped_df\n",
    "\n",
    "# Apply the optimized function to the DataFrame\n",
    "collapsed_adt2, dropped_df = drop_intermediate_er2(collapsed_adt)\n",
    "\n",
    "# Output the results\n",
    "print(\"Number of unique hospitalization_id:\", collapsed_adt2[\"hospitalization_id\"].nunique())\n",
    "print(\"Number of rows in df:\", len(collapsed_adt2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sankey_df = collapsed_adt2.reset_index().loc[:,['hospitalization_id','segment_rank','location_category_death']]\n",
    "sankey_df2 = sankey_df.loc[sankey_df.segment_rank.notna()] \n",
    "sankey_df3 = sankey_df2.pivot(\n",
    "    index=['hospitalization_id'],\n",
    "    columns='segment_rank',\n",
    "    values='location_category_death'\n",
    ").reset_index().fillna('Discharged')\n",
    "sankey_df4 = sankey_df3.iloc[:, :8] #only take first 7 locations, otherwise gets too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_death(df):\n",
    "    # List of columns that represent the different segments\n",
    "    segment_cols = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
    "\n",
    "    # Define a function to propagate 'Died' across the columns\n",
    "    def propagate(row):\n",
    "        death_found = False\n",
    "        for col in segment_cols:\n",
    "            if row[col] == 'Died':\n",
    "                death_found = True\n",
    "            if death_found:\n",
    "                row[col] = 'Died'\n",
    "        return row\n",
    "\n",
    "    # Apply the function to each row\n",
    "    df[segment_cols] = df[segment_cols].apply(propagate, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to propagate 'Death' to subsequent columns\n",
    "sankey_df4 = propagate_death(sankey_df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = {\n",
    "    \"ICU\": 'lightcoral',\n",
    "    \"Ward\": 'skyblue',\n",
    "    'Procedural':'thistle',\n",
    "    \"Discharged\": 'lightgrey',\n",
    "    \"Died\": 'grey',\n",
    "    \"ER\":'red'\n",
    "}\n",
    "fig, ax = plt.subplots(figsize=(14, 6), constrained_layout=True)\n",
    "diag = Sankey(\n",
    "    sankey_df4.iloc[:, 1:], \n",
    "    ax=ax, \n",
    "    order=[\"ICU\", \"Ward\", \"Procedural\", \"Discharged\",\"ER\",\"Died\"],\n",
    "    block_width=0.2,\n",
    "    colors=colors,\n",
    "    alpha=0.5\n",
    ")\n",
    "diag.draw()\n",
    "ax.set_title(\"\", size=16)\n",
    "ax.set_xticks(\n",
    "    [diag.block_width / 2 + diag.flow_width * x + diag.block_width * x for x in range(sankey_df4.shape[1] - 1)]\n",
    ")\n",
    "ax.set_xticklabels(sankey_df4.columns[1:].astype(int))\n",
    "ax.set_xlabel(\"Location number\", size=14)\n",
    "ax.get_xaxis().set_visible(True)\n",
    "ax.tick_params(axis=\"x\", pad=5, labelsize=16)\n",
    "\n",
    "# Export the figure as a high-resolution image\n",
    "fig.savefig(f'{output_path}/sankey_diagram_{site_name}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Table One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unplanned_icu_final = icu_final[icu_final[\"procedural\"]==False]\n",
    "#admission location\n",
    "admission_location = icu_df7[[\"hospitalization_id\", \"location_category\"]].groupby('hospitalization_id', as_index=False).first()\n",
    "unplanned_icu_final = pd.merge(unplanned_icu_final,admission_location,on=\"hospitalization_id\",how=\"left\")\n",
    "\n",
    "#length of ICU stay \n",
    "icu_sorted = icu_df7.sort_values(by=['hospitalization_id', 'earliest_location_start'])\n",
    "icu_first_icu = icu_sorted[icu_sorted['location_category'] == 'ICU'].groupby('hospitalization_id', as_index=False).first()\n",
    "length_ICU = icu_first_icu[['hospitalization_id', 'location_hours']]\n",
    "unplanned_icu_final = pd.merge(unplanned_icu_final,length_ICU,on=\"hospitalization_id\",how=\"left\")\n",
    "unplanned_icu_final['location_days'] = unplanned_icu_final['location_hours'] / 24\n",
    "\n",
    "#in-hospital mortality\n",
    "dead_patients = collapsed_adt2[collapsed_adt2[\"location_category_death\"]==\"Died\"][[\"hospitalization_id\",\"location_category_death\"]]\n",
    "unplanned_icu_final = pd.merge(unplanned_icu_final,dead_patients,on=\"hospitalization_id\",how=\"left\")\n",
    "unplanned_icu_final['location_category_death'] = unplanned_icu_final['location_category_death'].fillna('Alive')\n",
    "\n",
    "#age at admission\n",
    "hosp = hosp.drop_duplicates(subset=[\"hospitalization_id\"])\n",
    "age = hosp[[\"hospitalization_id\",\"age_at_admission\"]]                           \n",
    "unplanned_icu_final = pd.merge(unplanned_icu_final,age,on=\"hospitalization_id\",how=\"left\")\n",
    "\n",
    "#demographics \n",
    "demog = patient[[\"patient_id\",\"race_category\",\"ethnicity_category\",\"sex_category\"]]\n",
    "unplanned_icu_final = pd.merge(unplanned_icu_final,demog,on=\"patient_id\",how=\"left\")\n",
    "unplanned_icu_final = unplanned_icu_final.drop_duplicates(subset=[\"hospitalization_id\"])\n",
    "\n",
    "#readmitted\n",
    "unplanned_icu_final['readmission'] = unplanned_icu_final['ICU_readmission_hour'].apply(\n",
    "    lambda x: 'Readmitted' if pd.notna(x) and x > 0 else 'Not Readmitted'\n",
    ")\n",
    "\n",
    "print(unplanned_icu_final[\"hospitalization_id\"].nunique())\n",
    "unplanned_icu_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_one_dat = unplanned_icu_final[[\"age_at_admission\",\n",
    "                                     \"sex_category\",\n",
    "                                     \"race_category\",\n",
    "                                    \"ethnicity_category\",\n",
    "                                    \"location_category\",\n",
    "                                     \"ICU_readmission_hour\",\n",
    "                                    \"location_days\",\n",
    "                                    \"location_category_death\",\n",
    "                                    \"readmission\"]]\n",
    "\n",
    "# Rename columns\n",
    "table_one_dat = table_one_dat.rename(columns={'age_at_admission': 'Age', \n",
    "                                              'sex_category': 'Sex', \n",
    "                                              'race_category': 'Race',\n",
    "                                             'ethnicity_category': 'Ethnicity',\n",
    "                                             'location_category': 'Initial hospital location',\n",
    "                                              'ICU_readmission_hour':'ICU readmission, hours',\n",
    "                                              \"location_days\":'ICU length of stay, days',\n",
    "                                              \"location_category_death\":'In-hospital Mortality'\n",
    "                                             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns that include both continuous and categorical variables\n",
    "columns = [\"Age\", \"ICU readmission, hours\", \"ICU length of stay, days\", \"Sex\", \"Race\", \"Ethnicity\", \"Initial hospital location\", \"In-hospital Mortality\"]\n",
    "\n",
    "# Define which columns are categorical\n",
    "categorical = [\"Sex\", \"Race\", \"Ethnicity\", \"Initial hospital location\", \"In-hospital Mortality\"]\n",
    "\n",
    "# Define which continuous variables are not normally distributed\n",
    "nonnormal = [\"Age\",\"ICU readmission, hours\",\"ICU length of stay, days\"]\n",
    "\n",
    "#groupby\n",
    "groupby = 'readmission'\n",
    "\n",
    "# Generate the TableOne object\n",
    "table1 = TableOne(data=table_one_dat, \n",
    "                  columns=columns, \n",
    "                  categorical=categorical, \n",
    "                  nonnormal=nonnormal, \n",
    "                  groupby=groupby)\n",
    "\n",
    "# Display the table\n",
    "\n",
    "table1.to_csv(f'{output_path}/table1_{site_name}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv')\n",
    "table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

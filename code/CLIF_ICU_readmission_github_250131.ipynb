{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import time\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sankey import Sankey\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import tableone\n",
    "try:\n",
    "    from tableone import TableOne, load_dataset\n",
    "except (ModuleNotFoundError, ImportError):\n",
    "    # install on Colab\n",
    "    !pip install tableone\n",
    "    from tableone import TableOne, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from utils import config\n",
    "config = config.load_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## please change site_name, tables_path, output_path, and file_type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access configuration parameters\n",
    "site_name = config['site_name']\n",
    "tables_path = config['tables_path']\n",
    "file_type = config['file_type']\n",
    "output_path = os.path.join(\"..\", \"output\", \"final\")\n",
    "intermediate_output_path = os.path.join(\"..\", \"output\", \"intermediate\")\n",
    "\n",
    "# Make sure the directory exists; if not, create it\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "if not os.path.exists(intermediate_output_path):\n",
    "    os.makedirs(intermediate_output_path)\n",
    "\n",
    "# Print the configuration parameters\n",
    "print(f\"Site Name: {site_name}\")\n",
    "print(f\"Tables Path: {tables_path}\")\n",
    "print(f\"Output path: {output_path}\")\n",
    "print(f\"Intermediate output path: {intermediate_output_path}\")\n",
    "print(f\"File Type: {file_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confirm that these are the correct paths\n",
    "adt_filepath = f\"{tables_path}/clif_adt.{file_type}\"\n",
    "hospitalization_filepath = f\"{tables_path}/clif_hospitalization.{file_type}\" #remove clean from filename if irrelevant\n",
    "patient_filepath = f\"{tables_path}/clif_patient.{file_type}\"\n",
    "medication_filepath = f\"{tables_path}/clif_medication_admin_continuous.{file_type}\"\n",
    "resp_support_filepath = f\"{tables_path}/clif_respiratory_support.{file_type}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath, filetype):\n",
    "    start_time = time.time()  # Record the start time\n",
    "    file_name = os.path.basename(filepath) \n",
    "    if filetype == 'csv':\n",
    "        df = pd.read_csv(filepath)\n",
    "    elif filetype == 'parquet':\n",
    "        table = pq.read_table(filepath)\n",
    "        df = table.to_pandas()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please provide either 'csv' or 'parquet'.\")\n",
    "    \n",
    "    end_time = time.time()  # Record the end time\n",
    "    load_time = end_time - start_time  # Calculate the loading time\n",
    "    \n",
    "    # Calculate the size of the loaded dataset in MB\n",
    "    dataset_size_mb = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "    print(f\"File name: {file_name}\")\n",
    "    print(f\"Time taken to load the dataset: {load_time:.2f} seconds\")\n",
    "    print(f\"Size of the loaded dataset: {dataset_size_mb:.2f} MB\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "adt = read_data(adt_filepath, file_type)\n",
    "hosp = read_data(hospitalization_filepath, file_type)\n",
    "patient = read_data(patient_filepath,file_type)\n",
    "medication = read_data(medication_filepath,file_type)\n",
    "resp_support = read_data(resp_support_filepath,file_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adt[\"location_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other sites should edit this code on hospital category as needed depending on how it is organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adt['hospital_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adt['hospital_category'] = adt['hospital_id'].str.lower().str.split().str[0]\n",
    "#adt.loc[adt['location_name'] == 'KH CVIS', 'location_category'] = 'procedural'\n",
    "adt['hospital_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of unique hospitalization_id:\",hosp[\"hospitalization_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_admission_dttm = hosp['admission_dttm'].isnull().sum()\n",
    "print(\"Missing admission_dttm:\", missing_admission_dttm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp2 = hosp[[\"patient_id\",\"hospitalization_id\",\"admission_dttm\",\"discharge_dttm\",\"age_at_admission\"]].copy()\n",
    "hosp2['admission_dttm'] = pd.to_datetime(hosp2['admission_dttm'])\n",
    "hosp2['discharge_dttm'] = pd.to_datetime(hosp2['discharge_dttm'])\n",
    "hosp2 = hosp2[(hosp2['admission_dttm'].dt.year >= 2020) & \n",
    "                   (hosp2['admission_dttm'].dt.year <= 2021) & \n",
    "                   (hosp2['age_at_admission'] >=18)&\n",
    "                    (hosp2['age_at_admission'] <=119)&\n",
    "                     (hosp2['hospitalization_id'].isin(adt[adt['location_category'].str.lower() == \"icu\"]['hospitalization_id'].unique()))]\n",
    "\n",
    "df = pd.merge(hosp2[[\"patient_id\",\"hospitalization_id\",\"admission_dttm\",\"discharge_dttm\"]],\n",
    "                  adt[[\"hospitalization_id\",\"in_dttm\",\"out_dttm\",\"location_category\",\"hospital_category\"]],\n",
    "             on=\"hospitalization_id\",how=\"left\")\n",
    "\n",
    "print(\"number of unique hospitalization_id:\",df[\"hospitalization_id\"].nunique())\n",
    "print(\"number of rows in df:\",len(df))\n",
    "\n",
    "df[\"location_category\"] = df[\"location_category\"].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepdown_hospitals = pd.DataFrame(df[df[\"location_category\"]==\"stepdown\"][\"hospital_category\"].value_counts()).reset_index()\n",
    "stepdown_hospitals.to_csv(f'{output_path}/stepdown_hospitals_{site_name}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv')\n",
    "stepdown_hospitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary to update the location_category\n",
    "location_mapping = {\n",
    "    'rehab': 'Other',\n",
    "    'l&d': 'Ward',\n",
    "    'psych': 'Other',\n",
    "    'radiology': 'Other',\n",
    "    'dialysis': 'Other',\n",
    "    'hospice': 'Other',\n",
    "    'stepdown':'ICU',\n",
    "    \"icu\": \"ICU\",\n",
    "    \"er\": \"ER\",\n",
    "    \"ward\":\"Ward\",\n",
    "    \"procedural\":\"Procedural\",\n",
    "    \"other\":\"Other\",\n",
    "    'imaging':'Other',\n",
    "    'ltach':'Other',\n",
    "    \"ed\":\"ER\"\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'location_category' column\n",
    "df['location_category'] = df['location_category'].replace(location_mapping)\n",
    "df = df[df['location_category']!= \"Other\"] #removing \"Other from the study\"\n",
    "print(df[\"location_category\"].value_counts())\n",
    "\n",
    "hospital_cat = df[[\"hospitalization_id\",\"in_dttm\",\"out_dttm\",\"hospital_category\"]]\n",
    "\n",
    "# Step 1: Sort by patient_id and admission_dttm\n",
    "hospital_block = df[[\"patient_id\",\"hospitalization_id\",\"admission_dttm\",\"discharge_dttm\"]]\n",
    "hospital_block = hospital_block.drop_duplicates()\n",
    "hospital_block = hospital_block.sort_values(by=[\"patient_id\", \"admission_dttm\"]).reset_index(drop=True)\n",
    "hospital_block = hospital_block[[\"patient_id\",\"hospitalization_id\",\"admission_dttm\",\"discharge_dttm\"]]\n",
    "\n",
    "# Step 2: Calculate the time difference between discharge_dttm and the next admission_dttm for the same patient\n",
    "hospital_block[\"next_admission_dttm\"] = hospital_block.groupby(\"patient_id\")[\"admission_dttm\"].shift(-1)\n",
    "hospital_block[\"discharge_to_next_admission_hrs\"] = (\n",
    "    (hospital_block[\"next_admission_dttm\"] - hospital_block[\"discharge_dttm\"]).dt.total_seconds() / 3600\n",
    ")\n",
    "\n",
    "# Step 3: Create linked6hrs column\n",
    "hospital_block[\"linked6hrs\"] = hospital_block[\"discharge_to_next_admission_hrs\"] < 6\n",
    "print(hospital_block[\"linked6hrs\"].value_counts())\n",
    "\n",
    "# Sort values to ensure correct order\n",
    "hospital_block = hospital_block.sort_values(by=[\"patient_id\", \"admission_dttm\"]).reset_index(drop=True)\n",
    "\n",
    "# Initialize `new_group` with row indices + 1\n",
    "hospital_block['linked_group'] = hospital_block.index + 1\n",
    "\n",
    "# Iteratively propagate the `new_group` values to the next row\n",
    "while True:\n",
    "    # Shift `new_group` up by one row\n",
    "    shifted = hospital_block['linked_group'].shift(-1)\n",
    "\n",
    "    # Update `new_group` where `linked6hrs` is True and the same `patient_id`\n",
    "    mask = hospital_block['linked6hrs'] & (hospital_block['patient_id'] == hospital_block['patient_id'].shift(-1))\n",
    "    hospital_block.loc[mask, 'linked_group'] = shifted[mask]\n",
    "\n",
    "    # Break if no further changes occur\n",
    "    if hospital_block['linked_group'].equals(hospital_block['linked_group'].bfill()):\n",
    "        break\n",
    "\n",
    "# Backward fill `new_group` for finalized group numbers\n",
    "hospital_block['linked_group'] = hospital_block['linked_group'].bfill(downcast='int')\n",
    "hospital_block = pd.merge(hospital_block,hospital_cat,how=\"left\",on=\"hospitalization_id\")\n",
    "hospital_block = hospital_block.sort_values(by=[\"patient_id\", \"admission_dttm\",\"in_dttm\",\"out_dttm\"]).reset_index(drop=True)\n",
    "hospital_block = hospital_block.drop_duplicates()\n",
    "\n",
    "hospital_block2 = hospital_block.groupby(['patient_id','linked_group']).agg(\n",
    "    admission_dttm=pd.NamedAgg(column='admission_dttm', aggfunc='min'),\n",
    "    discharge_dttm=pd.NamedAgg(column='discharge_dttm', aggfunc='max'),\n",
    "    hospital_category = pd.NamedAgg(column='hospital_category', aggfunc='last'),\n",
    "    list_hospitalization_id=pd.NamedAgg(column='hospitalization_id', aggfunc=lambda x: sorted(x.unique()))\n",
    ").reset_index()\n",
    "\n",
    "df = pd.merge(hospital_block[[\"patient_id\",\n",
    "                              \"hospitalization_id\",\n",
    "                              \"linked_group\"]].drop_duplicates(),\n",
    "         df[[\"hospitalization_id\",\"location_category\",\"in_dttm\",\"out_dttm\"]], on=\"hospitalization_id\",how=\"left\")\n",
    "\n",
    "df = pd.merge(df,hospital_block2[[\"linked_group\",\n",
    "                                  \"admission_dttm\",\n",
    "                                  \"discharge_dttm\",\n",
    "                                  \"hospital_category\",\n",
    "                                 \"list_hospitalization_id\"]],on=\"linked_group\",how=\"left\")\n",
    "df = df.drop_duplicates(subset=[\"patient_id\",\"linked_group\",\"in_dttm\",\"out_dttm\",\"location_category\"])\n",
    "df_idlist = df[[\"patient_id\",\"hospitalization_id\"]].drop_duplicates()\n",
    "hospitalization_ids_to_remove = df[df['hospital_category'].isin([\"woodstock\", \"valley\"])]['linked_group'].unique()\n",
    "df = df[~df['linked_group'].isin(hospitalization_ids_to_remove)]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of encounters before linking: {df[\"hospitalization_id\"].nunique()}')\n",
    "print(f'Number of encounters after linking the transfer encounters together: {df[\"linked_group\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sort by linked_group and in_dttm\n",
    "df = df.sort_values(by=['linked_group', 'in_dttm', 'out_dttm'])\n",
    "#stitch together ICUs if ICU --> procedure --> ICU\n",
    "df['shifted_location'] = df['location_category'].shift(1)\n",
    "df['new_location'] = (df['location_category'] != df['shifted_location']).cumsum()\n",
    "df = df.groupby(['linked_group','new_location']).agg(\n",
    "    in_dttm=pd.NamedAgg(column='in_dttm', aggfunc='min'),\n",
    "    out_dttm=pd.NamedAgg(column='out_dttm', aggfunc='max'),\n",
    "    location_category=pd.NamedAgg(column='location_category', aggfunc='first'), \n",
    "    patient_id=pd.NamedAgg(column='patient_id', aggfunc='first')\n",
    ").reset_index()\n",
    "\n",
    "df.sort_values(['linked_group', 'in_dttm'], inplace=True)  # Replace 'timestamp_column' with the actual column name that orders your events chronologically.\n",
    "# Use groupby and shift to get previous and next 'location_category' within each hospitalization_id\n",
    "df['prev_category'] = df.groupby('linked_group')['location_category'].shift(1)\n",
    "df['next_category'] = df.groupby('linked_group')['location_category'].shift(-1)\n",
    "# Create a boolean mask for 'Procedural' rows sandwiched between 'ICU' rows\n",
    "mask = (\n",
    "    (df['location_category'] == 'Procedural') &\n",
    "    (df['prev_category'] == 'ICU') &\n",
    "    (df['next_category'] == 'ICU')\n",
    ")\n",
    "# Remove the rows where the mask is True\n",
    "df = df[~mask].reset_index(drop=True)\n",
    "# Drop the helper columns\n",
    "df.drop(columns=['prev_category', 'next_category'], inplace=True)\n",
    "\n",
    "print(\"Number of unique linked hospitalization_id:\", df[\"linked_group\"].nunique())\n",
    "print(\"Number of rows in df:\",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_stays(df):\n",
    "    # Ensure the in_dttm and out_dttm columns are in datetime format\n",
    "    df['in_dttm'] = pd.to_datetime(df['in_dttm'], errors='coerce')\n",
    "    df['out_dttm'] = pd.to_datetime(df['out_dttm'], errors='coerce')\n",
    "\n",
    "    # Directly filter rows based on the time difference without creating a new column\n",
    "    mask = (df['out_dttm'] - df['in_dttm']) >= pd.Timedelta(hours=1)\n",
    "    df_filtered = df[mask]\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# Sort the DataFrame once\n",
    "df = df.sort_values(by=['linked_group', 'in_dttm', 'out_dttm'])\n",
    "\n",
    "# Remove short stays\n",
    "df = remove_short_stays(df)\n",
    "\n",
    "# Efficiently shift and calculate new locations\n",
    "df['shifted_location'] = df['location_category'].shift(1)\n",
    "df['new_location'] = (df['location_category'] != df['shifted_location']).cumsum()\n",
    "\n",
    "# Group by hospitalization_id and new_location in one operation, minimize memory use\n",
    "df = df.groupby(['linked_group', 'new_location'], as_index=False).agg(\n",
    "    in_dttm=('in_dttm', 'min'),\n",
    "    out_dttm=('out_dttm', 'max'),\n",
    "    location_category=('location_category', 'first'),\n",
    "    patient_id=('patient_id', 'first')\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(\"Number of unique linked hospitalization_id:\", df[\"linked_group\"].nunique())\n",
    "print(\"Number of rows in df:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert in_dttm and out_dttm columns to datetime format\n",
    "df['in_dttm'] = pd.to_datetime(df['in_dttm'])\n",
    "df['out_dttm'] = pd.to_datetime(df['out_dttm'])\n",
    "\n",
    "# Sort the DataFrame\n",
    "df = df.sort_values(by=['linked_group', 'in_dttm', 'out_dttm'])\n",
    "\n",
    "# Function to mark rows as procedural if 'Procedural' is immediately before the second ICU\n",
    "def mark_procedural_before_second_icu(df):\n",
    "    # Find ICU rows\n",
    "    df['is_icu'] = df['location_category'] == 'ICU'\n",
    "    \n",
    "    # Create a group-level cumulative sum to count ICU admissions\n",
    "    df['icu_count'] = df.groupby(['patient_id', 'linked_group'])['is_icu'].cumsum()\n",
    "\n",
    "    # Create a 'procedural' column that is initially False\n",
    "    df['procedural'] = False\n",
    "\n",
    "    # Identify rows where the second ICU occurs\n",
    "    for hosp_id, group in df.groupby(['patient_id', 'linked_group']):\n",
    "        # Find the indices of ICU rows\n",
    "        icu_rows = group[group['location_category'] == 'ICU'].index\n",
    "\n",
    "        # If there are at least two ICU admissions\n",
    "        if len(icu_rows) > 1:\n",
    "            second_icu_index = icu_rows[1]  # Second ICU occurrence\n",
    "            \n",
    "            # Check the row immediately before the second ICU\n",
    "            if second_icu_index - 1 in group.index and group.loc[second_icu_index - 1, 'location_category'] == 'Procedural':\n",
    "                # Mark the row as procedural\n",
    "                df.loc[second_icu_index - 1, 'procedural'] = True\n",
    "                \n",
    "    # Propagate procedural == True to all rows in the same hospitalization_id if any row is marked as True\n",
    "    df['procedural'] = df.groupby('linked_group')['procedural'].transform('max')\n",
    "\n",
    "    # Drop helper columns\n",
    "    df.drop(columns=['is_icu', 'icu_count'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Further transformations as needed\n",
    "df2 = df.copy()\n",
    "\n",
    "# Ensure dataset is aggregated correctly\n",
    "df2['shifted_location'] = df2['location_category'].shift(1)\n",
    "df2['new_location'] = (df2['location_category'] != df2['shifted_location']).cumsum()\n",
    "\n",
    "df2 = df2.groupby(['linked_group', 'new_location']).agg(\n",
    "    in_dttm=('in_dttm', 'min'),\n",
    "    out_dttm=('out_dttm', 'max'),\n",
    "    location_category=('location_category', 'first'),\n",
    "    patient_id=('patient_id', 'first')\n",
    ").reset_index()\n",
    "\n",
    "# Apply the function to mark procedural rows\n",
    "df2 = mark_procedural_before_second_icu(df2)\n",
    "\n",
    "# Output the updated DataFrame\n",
    "print(\"Number of unique linked hospitalization_id:\", df2[\"linked_group\"].nunique())\n",
    "print(\"Number of rows in df:\", len(df2))\n",
    "print(\"Number of linked hospitalizations marked as procedural:\", df2[df2[\"procedural\"] == True][\"linked_group\"].nunique())\n",
    "df2[['patient_id', 'linked_group', 'location_category', 'procedural']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'in_dttm' and 'out_dttm' columns to datetime format (vectorized)\n",
    "df2['in_dttm'] = pd.to_datetime(df2['in_dttm'])\n",
    "df2['out_dttm'] = pd.to_datetime(df2['out_dttm'])\n",
    "\n",
    "# Sort the DataFrame by hospitalization_id and datetime columns\n",
    "df2 = df2.sort_values(by=['linked_group', 'in_dttm', 'out_dttm'])\n",
    "\n",
    "# Filter directly for ICU visits\n",
    "icu_visits = df2[df2['location_category'] == \"ICU\"]\n",
    "\n",
    "# Get the unique hospitalization_ids that had an ICU visit\n",
    "icu_visit_ids = icu_visits['linked_group'].unique()\n",
    "\n",
    "# Filter the entire DataFrame to include only rows with those hospitalization_ids\n",
    "icu_df = df2[df2['linked_group'].isin(icu_visit_ids)]\n",
    "\n",
    "# Sort the filtered DataFrame again if needed\n",
    "icu_df = icu_df.sort_values(by=['linked_group', 'in_dttm', 'out_dttm'])\n",
    "\n",
    "# Output the resulting DataFrame and summary statistics\n",
    "print(\"Number of unique linked hospitalization_id:\", icu_df[\"linked_group\"].nunique())\n",
    "print(\"Number of rows in df:\", len(icu_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_deaths_during_icu(df, patient_df):\n",
    "    # Merge death_dttm from patient data into icu_df, only keep relevant columns in patient_df\n",
    "    df = df.merge(patient_df[['patient_id', 'death_dttm']], on='patient_id', how='left')\n",
    "\n",
    "    # Convert 'death_dttm', 'in_dttm', and 'out_dttm' to datetime format (more efficient bulk conversion)\n",
    "    df[['death_dttm', 'in_dttm', 'out_dttm']] = df[['death_dttm', 'in_dttm', 'out_dttm']].apply(pd.to_datetime, errors='coerce')\n",
    "\n",
    "    # Filter directly for the first ICU admission per hospitalization_id\n",
    "    icu_first_admission = df[df['location_category'] == 'ICU'].sort_values('in_dttm').drop_duplicates('linked_group')\n",
    "\n",
    "    # Identify hospitalization_id where the patient died during ICU admission\n",
    "    death_during_icu = icu_first_admission.loc[\n",
    "        (icu_first_admission['death_dttm'].notnull()) &\n",
    "        (icu_first_admission['death_dttm'] >= icu_first_admission['in_dttm']) &\n",
    "        (icu_first_admission['death_dttm'] <= icu_first_admission['out_dttm']),\n",
    "        'linked_group'\n",
    "    ]\n",
    "\n",
    "    # Filter out rows where the patient died during ICU admission\n",
    "    df_cleaned = df[~df['linked_group'].isin(death_during_icu)]\n",
    "    dropped_df = df[df['linked_group'].isin(death_during_icu)]\n",
    "\n",
    "    return df_cleaned, dropped_df\n",
    "\n",
    "# Sorting the icu_df by 'hospitalization_id' and 'in_dttm'\n",
    "icu_df = icu_df.sort_values(by=['linked_group', 'in_dttm'])\n",
    "\n",
    "# Apply the function\n",
    "icu_df2, dropped_deaths_df = drop_deaths_during_icu(icu_df, patient)\n",
    "\n",
    "# Output statistics\n",
    "print(\"Number of unique linked hospitalization_id with death during ICU:\", dropped_deaths_df[\"linked_group\"].nunique())\n",
    "print(\"Number of unique linked hospitalization_id after removing death during ICU:\", icu_df2[\"linked_group\"].nunique())\n",
    "print(\"Number of rows in df:\", len(icu_df2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_direct_icu_discharges(df):\n",
    "    # Sort the DataFrame by hospitalization_id and in_dttm if not already sorted\n",
    "    df = df.sort_values(by=['linked_group', 'in_dttm'])\n",
    "\n",
    "    # Identify the first ICU admission for each hospitalization_id\n",
    "    first_icu = df[df['location_category'] == 'ICU'].groupby('linked_group').head(1)\n",
    "\n",
    "    # Identify the last row for each hospitalization_id\n",
    "    last_row = df.groupby('linked_group').tail(1)\n",
    "\n",
    "    # Correct the logic: Check where the first ICU row is also the last row in the hospitalization\n",
    "    visits_to_drop = first_icu[first_icu.index.isin(last_row.index)]['linked_group']\n",
    "\n",
    "    # Filter out the rows for hospitalizations where patients were discharged directly from ICU\n",
    "    df_cleaned = df[~df['linked_group'].isin(visits_to_drop)]\n",
    "    dropped_df = df[df['linked_group'].isin(visits_to_drop)]\n",
    "\n",
    "    # Drop helper columns (if any)\n",
    "    df_cleaned.drop(columns=['is_icu'], inplace=True, errors='ignore')\n",
    "\n",
    "    return df_cleaned, dropped_df\n",
    "\n",
    "# Sort the input DataFrame\n",
    "icu_df2 = icu_df2.sort_values(by=['linked_group', 'in_dttm', 'out_dttm'])\n",
    "\n",
    "# Apply the optimized function\n",
    "icu_df3, dropped_df_icu_discharge = drop_direct_icu_discharges(icu_df2)\n",
    "\n",
    "# Output the counts and dropped DataFrame\n",
    "print(\"Number of unique linked hospitalization_id after filtering:\", icu_df3[\"linked_group\"].nunique())\n",
    "print(\"Number of rows in df:\", len(icu_df3))\n",
    "print(\"Number of unique patient_id dropped:\", dropped_df_icu_discharge[\"patient_id\"].nunique())\n",
    "print(\"Number of unique linked hospitalization_id dropped:\", dropped_df_icu_discharge[\"linked_group\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redundant but making sure that the location_category is grouped together \n",
    "icu_df4 = icu_df3.sort_values(by=['linked_group', 'in_dttm','out_dttm'])\n",
    "icu_df4['shifted_location'] = icu_df4['location_category'].shift(1)\n",
    "icu_df4['new_location'] = (icu_df4['location_category'] != icu_df4['shifted_location']).cumsum()\n",
    "\n",
    "icu_df4 = icu_df4.groupby(['linked_group','new_location']).agg(\n",
    "    earliest_location_start=pd.NamedAgg(column='in_dttm', aggfunc='min'),\n",
    "    latest_location_end=pd.NamedAgg(column='out_dttm', aggfunc='max'),\n",
    "    location_category=pd.NamedAgg(column='location_category', aggfunc='first'), \n",
    "    patient_id=pd.NamedAgg(column='patient_id', aggfunc='first'),\n",
    "    procedural = pd.NamedAgg(column='procedural', aggfunc='first')\n",
    ").reset_index()\n",
    "\n",
    "print(\"Number of unique linked hospitalization_id:\", icu_df4[\"linked_group\"].nunique())\n",
    "icu_df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_intermediate_er(df):\n",
    "    # Sort the DataFrame to ensure it's ordered correctly by hospitalization_id and location timing\n",
    "    df = df.sort_values(by=['linked_group', 'earliest_location_start', 'latest_location_end'])\n",
    "\n",
    "    # Create a mask for rows that are the first row within each hospitalization_id group\n",
    "    first_row_mask = df.groupby('linked_group').cumcount() == 0\n",
    "\n",
    "    # Create a mask for rows where the location_category is 'ER'\n",
    "    er_mask = df['location_category'] == 'ER'\n",
    "\n",
    "    # Find the hospitalization_ids where 'ER' occurs after the first row\n",
    "    er_after_first_row = df[~first_row_mask & er_mask]['linked_group'].unique()\n",
    "\n",
    "    # Filter the DataFrame to drop those hospitalization_ids\n",
    "    df_cleaned = df[~df['linked_group'].isin(er_after_first_row)]\n",
    "    dropped_df = df[df['linked_group'].isin(er_after_first_row)]\n",
    "\n",
    "    return df_cleaned, dropped_df\n",
    "\n",
    "# Sort and apply the drop function\n",
    "icu_df4 = icu_df4.sort_values(by=['linked_group', 'earliest_location_start', 'latest_location_end'])\n",
    "icu_df5, dropped_df = drop_intermediate_er(icu_df4)\n",
    "\n",
    "# Convert to datetime and calculate location hours\n",
    "icu_df5['earliest_location_start'] = pd.to_datetime(icu_df5['earliest_location_start'])\n",
    "icu_df5['latest_location_end'] = pd.to_datetime(icu_df5['latest_location_end'])\n",
    "icu_df5['location_hours'] = (icu_df5['latest_location_end'] - icu_df5['earliest_location_start']).dt.total_seconds() / 3600\n",
    "\n",
    "# Output the results\n",
    "print(\"Number of rows in df:\", len(icu_df5))\n",
    "print(\"Number of unique linked hospitalization_id:\", icu_df5[\"linked_group\"].nunique())\n",
    "icu_df5.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICU readmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_icu_readmission_time(df):\n",
    "    # Initialize a new column 'ICU_readmission_hour' with None\n",
    "    df['ICU_readmission_hour'] = None\n",
    "\n",
    "    # Function to process each group\n",
    "    def process_group(group):\n",
    "        # Find the rows where location_category is ICU\n",
    "        icu_rows = group[group['location_category'] == 'ICU']\n",
    "\n",
    "        # Check if there are at least two ICU events\n",
    "        if len(icu_rows) > 1:\n",
    "            # Sort by earliest_start2 to ensure correct order\n",
    "            icu_rows = icu_rows.sort_values(by='earliest_location_start')\n",
    "\n",
    "            # Get the first and second ICU event\n",
    "            first_icu = icu_rows.iloc[0]\n",
    "            second_icu = icu_rows.iloc[1]\n",
    "\n",
    "            # Calculate the time difference between first and second ICU events in hours\n",
    "            time_diff = (second_icu['earliest_location_start'] - first_icu['latest_location_end']).total_seconds() / 3600\n",
    "\n",
    "            # Update the entire group with the calculated time difference\n",
    "            df.loc[group.index, 'ICU_readmission_hour'] = time_diff\n",
    "\n",
    "    # Apply the function to each group of 'person_id' and 'custom_visit_occurrence2'\n",
    "    df.groupby(['patient_id', 'linked_group']).apply(process_group)\n",
    "\n",
    "    return df\n",
    "\n",
    "icu_df6 = calculate_icu_readmission_time(icu_df5)\n",
    "print(\"number of rows in df:\",len(icu_df6))\n",
    "print(\"Number of unique hospitalization_id:\", icu_df6[\"linked_group\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify hospitalization_ids where ICU_readmission_hour is less than 0\n",
    "#This is in place incase there are errors in the adt, should be a very small number if any \n",
    "hosp_ids_to_drop = icu_df6[icu_df6[\"ICU_readmission_hour\"] <=0][\"linked_group\"].unique()\n",
    "\n",
    "# Drop rows with those hospitalization_ids\n",
    "icu_df7 = icu_df6[~icu_df6[\"linked_group\"].isin(hosp_ids_to_drop)]\n",
    "\n",
    "# Check the result\n",
    "print(\"Number of unique linked hospitalization_id after filtering:\", icu_df7[\"linked_group\"].nunique())\n",
    "print(\"Number of rows in df after filtering:\", len(icu_df7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by 'hospitalization_id' and 'in_dttm'\n",
    "#icu_df_export = icu_df7[icu_df7[\"procedural\"]==False].sort_values(by=['linked_group', 'earliest_location_start'])\n",
    "icu_df_export = icu_df7.sort_values(by=['linked_group', 'earliest_location_start'])\n",
    "\n",
    "# Convert earliest_location_start to datetime\n",
    "icu_df_export[\"earliest_location_start\"] = pd.to_datetime(icu_df_export[\"earliest_location_start\"])\n",
    "\n",
    "# Sort by hospitalization_id and earliest_location_start\n",
    "icu_df_export = icu_df_export.sort_values(by=[\"linked_group\", \"earliest_location_start\"])\n",
    "\n",
    "# Filter for ICU rows\n",
    "icu_df_export = icu_df_export[icu_df_export[\"location_category\"] == \"ICU\"]\n",
    "\n",
    "# Add a rank for ICU occurrences within each hospitalization_id\n",
    "icu_df_export[\"icu_rank\"] = icu_df_export.groupby(\"linked_group\").cumcount()\n",
    "\n",
    "# Filter for the first ICU (rank == 0)\n",
    "first_icu = icu_df_export[icu_df_export[\"icu_rank\"] == 0]\n",
    "first_icu = first_icu.drop(columns=[\"icu_rank\"])\n",
    "\n",
    "# Filter for the second ICU (rank == 1)\n",
    "second_icu = icu_df_export[icu_df_export[\"icu_rank\"] == 1]\n",
    "\n",
    "# Drop the rank column if no longer needed\n",
    "second_icu = second_icu.drop(columns=[\"icu_rank\"])\n",
    "second_icu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_support[\"device_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_support_important = resp_support[resp_support[\"device_category\"].isin([\"IMV\", \"NIPPV\", \"High Flow NC\"])]\n",
    "resp_support_important = resp_support_important[[\"hospitalization_id\",\"recorded_dttm\",\"device_category\"]]\n",
    "resp_support_important.head()\n",
    "\n",
    "first_icu = pd.merge(first_icu,df_idlist,on=\"patient_id\",how=\"left\")\n",
    "second_icu = pd.merge(second_icu,df_idlist,on=\"patient_id\",how=\"left\")\n",
    "\n",
    "resp_support_merged1_12hr = pd.merge(first_icu, resp_support_important, on=\"hospitalization_id\")\n",
    "\n",
    "resp_support_merged1_12hr = resp_support_merged1_12hr[\n",
    "    (resp_support_merged1_12hr[\"recorded_dttm\"] >= resp_support_merged1_12hr[\"earliest_location_start\"] ) &\n",
    "    (resp_support_merged1_12hr[\"recorded_dttm\"] <= resp_support_merged1_12hr[\"earliest_location_start\"] + pd.Timedelta(hours=12))\n",
    "]\n",
    "\n",
    "resp_support_merged1_12hr = resp_support_merged1_12hr.rename(columns={\"device_category\": \"device_category_first_icu_12hr\"})\n",
    "\n",
    "resp_support_merged2_12hr = pd.merge(second_icu, resp_support_important, on=\"hospitalization_id\")\n",
    "\n",
    "resp_support_merged2_12hr = resp_support_merged2_12hr[\n",
    "    (resp_support_merged2_12hr[\"recorded_dttm\"] >= resp_support_merged2_12hr[\"earliest_location_start\"] ) &\n",
    "    (resp_support_merged2_12hr[\"recorded_dttm\"] <= resp_support_merged2_12hr[\"earliest_location_start\"] + pd.Timedelta(hours=12))\n",
    "]\n",
    "\n",
    "resp_support_merged2_12hr = resp_support_merged2_12hr.rename(columns={\"device_category\": \"device_category_second_icu_12hr\"})\n",
    "\n",
    "resp_support_merged1_24hr = pd.merge(first_icu, resp_support_important, on=\"hospitalization_id\")\n",
    "\n",
    "resp_support_merged1_24hr = resp_support_merged1_24hr[\n",
    "    (resp_support_merged1_24hr[\"recorded_dttm\"] >= resp_support_merged1_24hr[\"earliest_location_start\"] ) &\n",
    "    (resp_support_merged1_24hr[\"recorded_dttm\"] <= resp_support_merged1_24hr[\"earliest_location_start\"] + pd.Timedelta(hours=24))\n",
    "]\n",
    "\n",
    "resp_support_merged1_24hr = resp_support_merged1_24hr.rename(columns={\"device_category\": \"device_category_first_icu_24hr\"})\n",
    "\n",
    "resp_support_merged2_24hr = pd.merge(second_icu, resp_support_important, on=\"hospitalization_id\")\n",
    "\n",
    "resp_support_merged2_24hr = resp_support_merged2_24hr[\n",
    "    (resp_support_merged2_24hr[\"recorded_dttm\"] >= resp_support_merged2_24hr[\"earliest_location_start\"] ) &\n",
    "    (resp_support_merged2_24hr[\"recorded_dttm\"] <= resp_support_merged2_24hr[\"earliest_location_start\"] + pd.Timedelta(hours=24))\n",
    "]\n",
    "\n",
    "resp_support_merged2_24hr = resp_support_merged2_24hr.rename(columns={\"device_category\": \"device_category_second_icu_24hr\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vasopressor_important = medication[medication[\"med_category\"].isin([\"norepinephrine\", \"epinephrine\", \"phenylephrine\", \"vasopressin\",\"angiotensin\"])]\n",
    "vasopressor_important = vasopressor_important[[\"hospitalization_id\",\"admin_dttm\",\"med_category\"]]\n",
    "vasopressor_important.head()\n",
    "\n",
    "vaso_merged1_12hr = pd.merge(first_icu, vasopressor_important, on=\"hospitalization_id\")\n",
    "\n",
    "# Apply the time condition (within 12 hours)\n",
    "vaso_merged1_12hr = vaso_merged1_12hr[\n",
    "    (vaso_merged1_12hr[\"admin_dttm\"] >= vaso_merged1_12hr[\"earliest_location_start\"]) &\n",
    "    (vaso_merged1_12hr[\"admin_dttm\"] <= vaso_merged1_12hr[\"earliest_location_start\"] + pd.Timedelta(hours=12))\n",
    "]\n",
    "\n",
    "vaso_merged1_12hr = vaso_merged1_12hr.rename(columns={\"med_category\": \"vaso_category_first_icu_12hr\"})\n",
    "\n",
    "vaso_merged2_12hr = pd.merge(second_icu, vasopressor_important, on=\"hospitalization_id\")\n",
    "\n",
    "vaso_merged2_12hr = vaso_merged2_12hr[\n",
    "    (vaso_merged2_12hr[\"admin_dttm\"] >= vaso_merged2_12hr[\"earliest_location_start\"]) &\n",
    "    (vaso_merged2_12hr[\"admin_dttm\"] <= vaso_merged2_12hr[\"earliest_location_start\"] + pd.Timedelta(hours=12))\n",
    "]\n",
    "\n",
    "vaso_merged2_12hr = vaso_merged2_12hr.rename(columns={\"med_category\": \"vaso_category_second_icu_12hr\"})\n",
    "\n",
    "vaso_merged1_24hr = pd.merge(first_icu, vasopressor_important, on=\"hospitalization_id\")\n",
    "\n",
    "# Apply the time condition (within 12 hours)\n",
    "vaso_merged1_24hr = vaso_merged1_24hr[\n",
    "    (vaso_merged1_24hr[\"admin_dttm\"] >= vaso_merged1_24hr[\"earliest_location_start\"]) &\n",
    "    (vaso_merged1_24hr[\"admin_dttm\"] <= vaso_merged1_24hr[\"earliest_location_start\"] + pd.Timedelta(hours=24))\n",
    "]\n",
    "\n",
    "vaso_merged1_24hr = vaso_merged1_24hr.rename(columns={\"med_category\": \"vaso_category_first_icu_24hr\"})\n",
    "\n",
    "vaso_merged2_24hr = pd.merge(second_icu, vasopressor_important, on=\"hospitalization_id\")\n",
    "\n",
    "vaso_merged2_24hr = vaso_merged2_24hr[\n",
    "    (vaso_merged2_24hr[\"admin_dttm\"] >= vaso_merged2_24hr[\"earliest_location_start\"]) &\n",
    "    (vaso_merged2_24hr[\"admin_dttm\"] <= vaso_merged2_24hr[\"earliest_location_start\"] + pd.Timedelta(hours=24))\n",
    "]\n",
    "\n",
    "vaso_merged2_24hr = vaso_merged2_24hr.rename(columns={\"med_category\": \"vaso_category_second_icu_24hr\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_support_merged1_12hr = resp_support_merged1_12hr[[\"linked_group\", \"device_category_first_icu_12hr\"]].drop_duplicates()\n",
    "resp_support_merged2_12hr = resp_support_merged2_12hr[[\"linked_group\", \"device_category_second_icu_12hr\"]].drop_duplicates()\n",
    "resp_support_merged1_24hr = resp_support_merged1_24hr[[\"linked_group\", \"device_category_first_icu_24hr\"]].drop_duplicates()\n",
    "resp_support_merged2_24hr = resp_support_merged2_24hr[[\"linked_group\", \"device_category_second_icu_24hr\"]].drop_duplicates()\n",
    "vaso_merged1_12hr = vaso_merged1_12hr[[\"linked_group\", \"vaso_category_first_icu_12hr\"]].drop_duplicates()\n",
    "vaso_merged2_12hr = vaso_merged2_12hr[[\"linked_group\", \"vaso_category_second_icu_12hr\"]].drop_duplicates()\n",
    "vaso_merged1_24hr = vaso_merged1_24hr[[\"linked_group\", \"vaso_category_first_icu_24hr\"]].drop_duplicates()\n",
    "vaso_merged2_24hr = vaso_merged2_24hr[[\"linked_group\", \"vaso_category_second_icu_24hr\"]].drop_duplicates()\n",
    "hospital_block2 = hospital_block2[[\"linked_group\", \"hospital_category\"]].drop_duplicates()\n",
    "\n",
    "icu_final = icu_df7[[\"patient_id\", \"linked_group\", \"ICU_readmission_hour\", \"procedural\"]]\n",
    "\n",
    "# Merge respiratory support\n",
    "device_merge = pd.merge(resp_support_merged1_12hr, resp_support_merged2_12hr, on=\"linked_group\", how=\"outer\")\n",
    "device_merge = pd.merge(device_merge, resp_support_merged1_24hr, on=\"linked_group\", how=\"outer\")\n",
    "device_merge = pd.merge(device_merge, resp_support_merged2_24hr, on=\"linked_group\", how=\"outer\")\n",
    "device_merge = device_merge.rename(columns={\n",
    "    \"device_category_first_icu_12hr\": \"resp_support_first_12hr\",\n",
    "    \"device_category_second_icu_12hr\": \"resp_support_second_12hr\",\n",
    "    \"device_category_first_icu_24hr\": \"resp_support_first_24hr\",\n",
    "    \"device_category_second_icu_24hr\": \"resp_support_second_24hr\"\n",
    "})\n",
    "device_merge = device_merge.drop_duplicates()\n",
    "icu_final = pd.merge(icu_final, device_merge, on=\"linked_group\", how=\"left\").drop_duplicates()\n",
    "\n",
    "# Merge vasopressors\n",
    "vaso_merge = pd.merge(vaso_merged1_12hr, vaso_merged2_12hr, on=\"linked_group\", how=\"outer\")\n",
    "vaso_merge = pd.merge(vaso_merge, vaso_merged1_24hr, on=\"linked_group\", how=\"outer\")\n",
    "vaso_merge = pd.merge(vaso_merge, vaso_merged2_24hr, on=\"linked_group\", how=\"outer\")\n",
    "vaso_merge = vaso_merge.rename(columns={\n",
    "    \"vaso_category_first_icu_12hr\": \"vaso_first_12hr\",\n",
    "    \"vaso_category_second_icu_12hr\": \"vaso_second_12hr\",\n",
    "    \"vaso_category_first_icu_24hr\": \"vaso_first_24hr\",\n",
    "    \"vaso_category_second_icu_24hr\": \"vaso_second_24hr\"\n",
    "})\n",
    "vaso_merge = vaso_merge.drop_duplicates()\n",
    "icu_final = pd.merge(icu_final, vaso_merge, on=\"linked_group\", how=\"left\").drop_duplicates()\n",
    "\n",
    "# Merge hospital block\n",
    "icu_final = pd.merge(icu_final, hospital_block2, on=\"linked_group\", how=\"left\").drop_duplicates()\n",
    "icu_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Count of Respiratory Support from 12 hours since ICU admission:\\n{icu_final[\"resp_support_first_12hr\"].value_counts()}')\n",
    "print(f'Count of Respiratory Support from 12 hours since ICU readmission:\\n{icu_final[\"resp_support_second_12hr\"].value_counts()}')\n",
    "print(f'Count of Respiratory Support from 24 hours since ICU admission:\\n {icu_final[\"resp_support_first_24hr\"].value_counts()}')   \n",
    "print(f'Count of Respiratory Support from 24 hours since ICU readmission:\\n{icu_final[\"resp_support_second_24hr\"].value_counts()}')\n",
    "print(f'Count of Vasopressor from 12 hours since ICU admission:\\n {icu_final[\"vaso_first_12hr\"].value_counts()}')   \n",
    "print(f'Count of Vasopressor from 12 hours since ICU readmission:\\n{icu_final[\"vaso_second_12hr\"].value_counts()}') \n",
    "print(f'Count of Vasopressor from 24 hours since ICU admission:\\n {icu_final[\"vaso_first_24hr\"].value_counts()}')   \n",
    "print(f'Count of Vasopressor from 24 hours since ICU readmission:\\n{icu_final[\"vaso_second_24hr\"].value_counts()}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ICU Readmission Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by hospital category\n",
    "hospital_groups = icu_final.groupby('hospital_category')\n",
    "\n",
    "# Initialize an empty list to store table rows\n",
    "table_data = []\n",
    "\n",
    "# Loop over each hospital group to collect data\n",
    "for hospital, group in hospital_groups:\n",
    "    hospital_name = hospital.lower().replace(\" \", \"_\")\n",
    "\n",
    "    # Calculations\n",
    "    total_unique_visits = group[\"linked_group\"].nunique()\n",
    "    unique_visits_with_procedures = group[group[\"procedural\"] == True][\"linked_group\"].nunique()\n",
    "    unique_visits_without_procedures = group[group[\"procedural\"] == False][\"linked_group\"].nunique()\n",
    "\n",
    "    all_icu_readmission = group[(group[\"ICU_readmission_hour\"] > 0)][\"linked_group\"].nunique()\n",
    "    all_icu_readmission_6hours = group[(group[\"ICU_readmission_hour\"] < 6)][\"linked_group\"].nunique()\n",
    "    all_icu_readmission_24hours = group[(group[\"ICU_readmission_hour\"] < 24)][\"linked_group\"].nunique()\n",
    "    all_icu_readmission_48hours = group[(group[\"ICU_readmission_hour\"] < 48)][\"linked_group\"].nunique()\n",
    "    all_icu_readmission_72hours = group[(group[\"ICU_readmission_hour\"] < 72)][\"linked_group\"].nunique()\n",
    "\n",
    "    unplanned_icu_readmission = group[(group[\"procedural\"] == False) & (group[\"ICU_readmission_hour\"] > 0)][\"linked_group\"].nunique()\n",
    "    unplanned_icu_readmission_6hours = group[(group[\"procedural\"] == False) & (group[\"ICU_readmission_hour\"] < 6)][\"linked_group\"].nunique()\n",
    "    unplanned_icu_readmission_24hours = group[(group[\"procedural\"] == False) & (group[\"ICU_readmission_hour\"] < 24)][\"linked_group\"].nunique()\n",
    "    unplanned_icu_readmission_48hours = group[(group[\"procedural\"] == False) & (group[\"ICU_readmission_hour\"] < 48)][\"linked_group\"].nunique()\n",
    "    unplanned_icu_readmission_72hours = group[(group[\"procedural\"] == False) & (group[\"ICU_readmission_hour\"] < 72)][\"linked_group\"].nunique()\n",
    "\n",
    "    # Percentages\n",
    "    all_icu_read = round((all_icu_readmission / total_unique_visits) * 100, 2)\n",
    "    all_6hrs = round((all_icu_readmission_6hours / total_unique_visits) * 100, 2)\n",
    "    all_24hrs = round((all_icu_readmission_24hours / total_unique_visits) * 100, 2)\n",
    "    all_48hrs = round((all_icu_readmission_48hours / total_unique_visits) * 100, 2)\n",
    "    all_72hrs = round((all_icu_readmission_72hours / total_unique_visits) * 100, 2)\n",
    "\n",
    "    unplanned_icu_read = round((unplanned_icu_readmission / unique_visits_without_procedures) * 100, 2) if unique_visits_without_procedures > 0 else 0\n",
    "    unplanned_6hrs = round((unplanned_icu_readmission_6hours / unique_visits_without_procedures) * 100, 2) if unique_visits_without_procedures > 0 else 0\n",
    "    unplanned_24hrs = round((unplanned_icu_readmission_24hours / unique_visits_without_procedures) * 100, 2) if unique_visits_without_procedures > 0 else 0\n",
    "    unplanned_48hrs = round((unplanned_icu_readmission_48hours / unique_visits_without_procedures) * 100, 2) if unique_visits_without_procedures > 0 else 0\n",
    "    unplanned_72hrs = round((unplanned_icu_readmission_72hours / unique_visits_without_procedures) * 100, 2) if unique_visits_without_procedures > 0 else 0\n",
    "\n",
    "    # Append a row for the current hospital\n",
    "    table_data.append({\n",
    "        \"Hospital\": hospital,\n",
    "        \"Total Unique Visits\": total_unique_visits,\n",
    "        \"Planned Readmission (Procedural)\": unique_visits_with_procedures,\n",
    "        \"Unplanned Readmission (Non-Procedural)\": unique_visits_without_procedures,\n",
    "        \"All ICU Readmission (%)\": all_icu_read,\n",
    "        \"Unplanned ICU Readmission (%)\": unplanned_icu_read,\n",
    "        \"Unplanned ICU Readmission <6hrs (%)\": unplanned_6hrs,\n",
    "        \"Unplanned ICU Readmission <24hrs (%)\": unplanned_24hrs,\n",
    "        \"Unplanned ICU Readmission <48hrs (%)\": unplanned_48hrs,\n",
    "        \"Unplanned ICU Readmission <72hrs (%)\": unplanned_72hrs\n",
    "    })\n",
    "\n",
    "# Convert the data to a DataFrame for better visualization\n",
    "icu_readmission_table = pd.DataFrame(table_data)\n",
    "icu_readmission_table.to_csv(f'{output_path}/icu_readmission_table_{site_name}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv')\n",
    "\n",
    "icu_readmission_table = icu_readmission_table.set_index(\"Hospital\")\n",
    "\n",
    "heatmap_data = icu_readmission_table[\n",
    "    [\n",
    "       # \"All ICU Readmission (%)\",\n",
    "        \"Unplanned ICU Readmission (%)\",\n",
    "        \"Unplanned ICU Readmission <6hrs (%)\",\n",
    "        \"Unplanned ICU Readmission <24hrs (%)\",\n",
    "        \"Unplanned ICU Readmission <48hrs (%)\",\n",
    "        \"Unplanned ICU Readmission <72hrs (%)\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Create the heatmap with a red color scheme\n",
    "plt.figure(figsize=(14, 10))  # Increased figure size for better layout\n",
    "ax = sns.heatmap(\n",
    "    heatmap_data, \n",
    "    annot=True, \n",
    "    cmap=\"BuPu\", \n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"ICU Readmission Rates by Hospital\", fontsize=16)\n",
    "plt.xlabel(\"Metrics\", fontsize=12)\n",
    "plt.ylabel(\"Hospital\", fontsize=12)\n",
    "\n",
    "ax.set_ylim(len(heatmap_data) + 0.5, - 0.5)\n",
    "\n",
    "plt.savefig(f'{intermediate_output_path}/icu_readmission_heatmap_{site_name}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_groups = icu_final.groupby('hospital_category')\n",
    "\n",
    "for hospital, group in hospital_groups:\n",
    "    hospital_name = hospital.lower().replace(\" \", \"_\")\n",
    "\n",
    "    # Calculations\n",
    "    total_unique_visits = group[\"linked_group\"].nunique()\n",
    "    unique_visits_with_procedures = group[group[\"procedural\"] == True][\"linked_group\"].nunique()\n",
    "    unique_visits_without_procedures = group[group[\"procedural\"] == False][\"linked_group\"].nunique()\n",
    "\n",
    "    all_icu_readmission = group[(group[\"ICU_readmission_hour\"] > 0)][\"linked_group\"].nunique()\n",
    "    all_icu_readmission_6hours = group[(group[\"ICU_readmission_hour\"] < 6)][\"linked_group\"].nunique()\n",
    "    all_icu_readmission_24hours = group[(group[\"ICU_readmission_hour\"] < 24)][\"linked_group\"].nunique()\n",
    "    all_icu_readmission_48hours = group[(group[\"ICU_readmission_hour\"] < 48)][\"linked_group\"].nunique()\n",
    "    all_icu_readmission_72hours = group[(group[\"ICU_readmission_hour\"] < 72)][\"linked_group\"].nunique()\n",
    "\n",
    "    unplanned_icu_readmission = group[(group[\"procedural\"] == False) & (group[\"ICU_readmission_hour\"] > 0)][\"linked_group\"].nunique()\n",
    "    unplanned_icu_readmission_6hours = group[(group[\"procedural\"] == False) & (group[\"ICU_readmission_hour\"] < 6)][\"linked_group\"].nunique()\n",
    "    unplanned_icu_readmission_24hours = group[(group[\"procedural\"] == False) & (group[\"ICU_readmission_hour\"] < 24)][\"linked_group\"].nunique()\n",
    "    unplanned_icu_readmission_48hours = group[(group[\"procedural\"] == False) & (group[\"ICU_readmission_hour\"] < 48)][\"linked_group\"].nunique()\n",
    "    unplanned_icu_readmission_72hours = group[(group[\"procedural\"] == False) & (group[\"ICU_readmission_hour\"] < 72)][\"linked_group\"].nunique()\n",
    "\n",
    "    # Percentages\n",
    "    all_icu_read = round((all_icu_readmission / total_unique_visits) * 100, 2)\n",
    "    all_6hrs = round((all_icu_readmission_6hours / total_unique_visits) * 100, 2)\n",
    "    all_24hrs = round((all_icu_readmission_24hours / total_unique_visits) * 100, 2)\n",
    "    all_48hrs = round((all_icu_readmission_48hours / total_unique_visits) * 100, 2)\n",
    "    all_72hrs = round((all_icu_readmission_72hours / total_unique_visits) * 100, 2)\n",
    "\n",
    "    unplanned_icu_read = round((unplanned_icu_readmission / unique_visits_without_procedures) * 100, 2) if unique_visits_without_procedures > 0 else 0\n",
    "    unplanned_6hrs = round((unplanned_icu_readmission_6hours / unique_visits_without_procedures) * 100, 2) if unique_visits_without_procedures > 0 else 0\n",
    "    unplanned_24hrs = round((unplanned_icu_readmission_24hours / unique_visits_without_procedures) * 100, 2) if unique_visits_without_procedures > 0 else 0\n",
    "    unplanned_48hrs = round((unplanned_icu_readmission_48hours / unique_visits_without_procedures) * 100, 2) if unique_visits_without_procedures > 0 else 0\n",
    "    unplanned_72hrs = round((unplanned_icu_readmission_72hours / unique_visits_without_procedures) * 100, 2) if unique_visits_without_procedures > 0 else 0\n",
    "\n",
    "    # Save as unique variables\n",
    "    globals()[f\"{hospital_name}_total_unique_visits\"] = total_unique_visits\n",
    "    globals()[f\"{hospital_name}_unique_visits_with_procedures\"] = unique_visits_with_procedures\n",
    "    globals()[f\"{hospital_name}_unique_visits_without_procedures\"] = unique_visits_without_procedures\n",
    "\n",
    "    globals()[f\"{hospital_name}_all_icu_readmission\"] = all_icu_readmission\n",
    "    globals()[f\"{hospital_name}_all_icu_readmission_6hours\"] = all_icu_readmission_6hours\n",
    "    globals()[f\"{hospital_name}_all_icu_readmission_24hours\"] = all_icu_readmission_24hours\n",
    "    globals()[f\"{hospital_name}_all_icu_readmission_48hours\"] = all_icu_readmission_48hours\n",
    "    globals()[f\"{hospital_name}_all_icu_readmission_72hours\"] = all_icu_readmission_72hours\n",
    "\n",
    "    globals()[f\"{hospital_name}_unplanned_icu_readmission\"] = unplanned_icu_readmission\n",
    "    globals()[f\"{hospital_name}_unplanned_icu_readmission_6hours\"] = unplanned_icu_readmission_6hours\n",
    "    globals()[f\"{hospital_name}_unplanned_icu_readmission_24hours\"] = unplanned_icu_readmission_24hours\n",
    "    globals()[f\"{hospital_name}_unplanned_icu_readmission_48hours\"] = unplanned_icu_readmission_48hours\n",
    "    globals()[f\"{hospital_name}_unplanned_icu_readmission_72hours\"] = unplanned_icu_readmission_72hours\n",
    "\n",
    "    globals()[f\"{hospital_name}_all_icu_read\"] = all_icu_read\n",
    "    globals()[f\"{hospital_name}_all_6hrs\"] = all_6hrs\n",
    "    globals()[f\"{hospital_name}_all_24hrs\"] = all_24hrs\n",
    "    globals()[f\"{hospital_name}_all_48hrs\"] = all_48hrs\n",
    "    globals()[f\"{hospital_name}_all_72hrs\"] = all_72hrs\n",
    "\n",
    "    globals()[f\"{hospital_name}_unplanned_icu_read\"] = unplanned_icu_read\n",
    "    globals()[f\"{hospital_name}_unplanned_6hrs\"] = unplanned_6hrs\n",
    "    globals()[f\"{hospital_name}_unplanned_24hrs\"] = unplanned_24hrs\n",
    "    globals()[f\"{hospital_name}_unplanned_48hrs\"] = unplanned_48hrs\n",
    "    globals()[f\"{hospital_name}_unplanned_72hrs\"] = unplanned_72hrs\n",
    "\n",
    "    # Output\n",
    "    print(f\"Hospital: {hospital}\")\n",
    "    print(f\"  Total unique hospitalization_id: {total_unique_visits}\")\n",
    "    print(f\"  Planned readmission (procedural): {unique_visits_with_procedures}\")\n",
    "    print(f\"  Unplanned readmission (non-procedural): {unique_visits_without_procedures}\")\n",
    "\n",
    "    print(f\"  All ICU readmission ({all_icu_readmission}/{total_unique_visits}): {all_icu_read}%\")\n",
    "    print(f\"  All ICU readmission <6hr ({all_icu_readmission_6hours}/{total_unique_visits}): {all_6hrs}%\")\n",
    "    print(f\"  All ICU readmission <24hr ({all_icu_readmission_24hours}/{total_unique_visits}): {all_24hrs}%\")\n",
    "    print(f\"  All ICU readmission <48hr ({all_icu_readmission_48hours}/{total_unique_visits}): {all_48hrs}%\")\n",
    "    print(f\"  All ICU readmission <72hr ({all_icu_readmission_72hours}/{total_unique_visits}): {all_72hrs}%\")\n",
    "\n",
    "    print(f\"  Unplanned ICU readmission ({unplanned_icu_readmission}/{unique_visits_without_procedures}): {unplanned_icu_read}%\")\n",
    "    print(f\"  Unplanned ICU readmission <6hr ({unplanned_icu_readmission_6hours}/{unique_visits_without_procedures}): {unplanned_6hrs}%\")\n",
    "    print(f\"  Unplanned ICU readmission <24hr ({unplanned_icu_readmission_24hours}/{unique_visits_without_procedures}): {unplanned_24hrs}%\")\n",
    "    print(f\"  Unplanned ICU readmission <48hr ({unplanned_icu_readmission_48hours}/{unique_visits_without_procedures}): {unplanned_48hrs}%\")\n",
    "    print(f\"  Unplanned ICU readmission <72hr ({unplanned_icu_readmission_72hours}/{unique_visits_without_procedures}): {unplanned_72hrs}%\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vasopressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by hospital category\n",
    "hospital_groups = icu_final.groupby('hospital_category')\n",
    "\n",
    "# Initialize an empty list to store table rows\n",
    "table_data = []\n",
    "\n",
    "# Loop over each hospital group to collect data\n",
    "for hospital, group in hospital_groups:\n",
    "    # Generate hospital-specific variable prefix\n",
    "    hospital_name = hospital.lower().replace(\" \", \"_\")\n",
    "\n",
    "    # Retrieve previously saved variables\n",
    "    total_unique_visits = globals().get(f\"{hospital_name}_total_unique_visits\", 0)\n",
    "    unplanned_icu_readmission = globals().get(f\"{hospital_name}_unplanned_icu_readmission\", 0)\n",
    "    unplanned_icu_readmission_6hours = globals().get(f\"{hospital_name}_unplanned_icu_readmission_6hours\", 0)\n",
    "    unplanned_icu_readmission_24hours = globals().get(f\"{hospital_name}_unplanned_icu_readmission_24hours\", 0)\n",
    "    unplanned_icu_readmission_48hours = globals().get(f\"{hospital_name}_unplanned_icu_readmission_48hours\", 0)\n",
    "    unplanned_icu_readmission_72hours = globals().get(f\"{hospital_name}_unplanned_icu_readmission_72hours\", 0)\n",
    "\n",
    "    # Current calculations\n",
    "    total_unique_visits = group[\"linked_group\"].nunique()\n",
    "    vasopressin_count_12hr = group[(group[\"vaso_first_12hr\"].isna() == False)][\"linked_group\"].nunique()\n",
    "    vasopressin_count_24hr = group[(group[\"vaso_first_24hr\"].isna() == False)][\"linked_group\"].nunique()\n",
    "    unplanned_firsticu_vasopressin_pct_12hr = round((vasopressin_count_12hr / total_unique_visits) * 100, 2)\n",
    "    unplanned_firsticu_vasopressin_pct_24hr = round((vasopressin_count_24hr / total_unique_visits) * 100, 2)\n",
    "\n",
    "    unplanned_secondicu_vasopressin_12hr = group[(group[\"procedural\"] == False) & (group[\"vaso_second_12hr\"].isna() == False)][\"linked_group\"].nunique()\n",
    "    unplanned_secondicu_vasopressin_6hr_12hr = group[(group[\"procedural\"] == False) & (group[\"vaso_second_12hr\"].isna() == False) & (group[\"ICU_readmission_hour\"] < 6)][\"linked_group\"].nunique()\n",
    "    unplanned_secondicu_vasopressin_24hr_12hr = group[(group[\"procedural\"] == False) & (group[\"vaso_second_12hr\"].isna() == False) & (group[\"ICU_readmission_hour\"] < 24)][\"linked_group\"].nunique()\n",
    "    unplanned_secondicu_vasopressin_48hr_12hr = group[(group[\"procedural\"] == False) & (group[\"vaso_second_12hr\"].isna() == False) & (group[\"ICU_readmission_hour\"] < 48)][\"linked_group\"].nunique()\n",
    "    unplanned_secondicu_vasopressin_72hr_12hr = group[(group[\"procedural\"] == False) & (group[\"vaso_second_12hr\"].isna() == False) & (group[\"ICU_readmission_hour\"] < 72)][\"linked_group\"].nunique()\n",
    "    \n",
    "    unplanned_secondicu_vasopressin_24hr = group[(group[\"procedural\"] == False) & (group[\"vaso_second_24hr\"].isna() == False)][\"linked_group\"].nunique()\n",
    "    unplanned_secondicu_vasopressin_6hr_24hr = group[(group[\"procedural\"] == False) & (group[\"vaso_second_24hr\"].isna() == False) & (group[\"ICU_readmission_hour\"] < 6)][\"linked_group\"].nunique()\n",
    "    unplanned_secondicu_vasopressin_24hr_24hr = group[(group[\"procedural\"] == False) & (group[\"vaso_second_24hr\"].isna() == False) & (group[\"ICU_readmission_hour\"] < 24)][\"linked_group\"].nunique()\n",
    "    unplanned_secondicu_vasopressin_48hr_24hr = group[(group[\"procedural\"] == False) & (group[\"vaso_second_24hr\"].isna() == False) & (group[\"ICU_readmission_hour\"] < 48)][\"linked_group\"].nunique()\n",
    "    unplanned_secondicu_vasopressin_72hr_24hr = group[(group[\"procedural\"] == False) & (group[\"vaso_second_24hr\"].isna() == False) & (group[\"ICU_readmission_hour\"] < 72)][\"linked_group\"].nunique()\n",
    "    \n",
    "    unplanned_secondicu_vasopressin_pct_12hr = round((unplanned_secondicu_vasopressin_12hr / unplanned_icu_readmission) * 100, 2) if unplanned_icu_readmission > 0 else 0\n",
    "    unplanned_secondicu_vasopressin_6hr_pct_12hr = round((unplanned_secondicu_vasopressin_6hr_12hr / unplanned_icu_readmission_6hours) * 100, 2) if unplanned_icu_readmission_6hours > 0 else 0\n",
    "    unplanned_secondicu_vasopressin_24hr_pct_12hr = round((unplanned_secondicu_vasopressin_24hr_12hr / unplanned_icu_readmission_24hours) * 100, 2) if unplanned_icu_readmission_24hours > 0 else 0\n",
    "    unplanned_secondicu_vasopressin_48hr_pct_12hr = round((unplanned_secondicu_vasopressin_48hr_12hr / unplanned_icu_readmission_48hours) * 100, 2) if unplanned_icu_readmission_48hours > 0 else 0\n",
    "    unplanned_secondicu_vasopressin_72hr_pct_12hr = round((unplanned_secondicu_vasopressin_72hr_12hr / unplanned_icu_readmission_72hours) * 100, 2) if unplanned_icu_readmission_72hours > 0 else 0\n",
    "    \n",
    "    unplanned_secondicu_vasopressin_pct_24hr = round((unplanned_secondicu_vasopressin_24hr / unplanned_icu_readmission) * 100, 2) if unplanned_icu_readmission > 0 else 0\n",
    "    unplanned_secondicu_vasopressin_6hr_pct_24hr = round((unplanned_secondicu_vasopressin_6hr_24hr / unplanned_icu_readmission_6hours) * 100, 2) if unplanned_icu_readmission_6hours > 0 else 0\n",
    "    unplanned_secondicu_vasopressin_24hr_pct_24hr = round((unplanned_secondicu_vasopressin_24hr_24hr / unplanned_icu_readmission_24hours) * 100, 2) if unplanned_icu_readmission_24hours > 0 else 0\n",
    "    unplanned_secondicu_vasopressin_48hr_pct_24hr = round((unplanned_secondicu_vasopressin_48hr_24hr / unplanned_icu_readmission_48hours) * 100, 2) if unplanned_icu_readmission_48hours > 0 else 0\n",
    "    unplanned_secondicu_vasopressin_72hr_pct_24hr = round((unplanned_secondicu_vasopressin_72hr_24hr / unplanned_icu_readmission_72hours) * 100, 2) if unplanned_icu_readmission_72hours > 0 else 0\n",
    "    \n",
    "    \n",
    "    # Append a row for the current hospital\n",
    "    table_data.append({\n",
    "        \"Hospital\": hospital,\n",
    "        \"First ICU: Vasopressor % (12hr since ICU admission)\": unplanned_firsticu_vasopressin_pct_12hr,\n",
    "        \"First ICU: Vasopressor % (24hr since ICU admission)\": unplanned_firsticu_vasopressin_pct_24hr,\n",
    "        \"Second ICU: Vasopressor % (12hr since any unplanned ICU readmission)\": unplanned_secondicu_vasopressin_pct_12hr,\n",
    "        \"Second ICU: Vasopressor % (12hr since <24hr unplanned ICU readmission)\": unplanned_secondicu_vasopressin_24hr_pct_12hr,\n",
    "        \"Second ICU: Vasopressor % (12hr since <48hr unplanned ICU readmission)\": unplanned_secondicu_vasopressin_48hr_pct_12hr,\n",
    "        \"Second ICU: Vasopressor % (12hr since <72hr unplanned ICU readmission)\": unplanned_secondicu_vasopressin_72hr_pct_12hr,\n",
    "        \"Second ICU: Vasopressor % (24hr since any unplanned ICU readmission)\": unplanned_secondicu_vasopressin_pct_24hr,\n",
    "        \"Second ICU: Vasopressor % (24hr since <24hr unplanned ICU readmission)\": unplanned_secondicu_vasopressin_24hr_pct_24hr,\n",
    "        \"Second ICU: Vasopressor % (24hr since <48hr unplanned ICU readmission)\": unplanned_secondicu_vasopressin_48hr_pct_24hr,\n",
    "        \"Second ICU: Vasopressor % (24hr since <72hr unplanned ICU readmission)\": unplanned_secondicu_vasopressin_72hr_pct_24hr,\n",
    "    })\n",
    "\n",
    "# Convert the data to a DataFrame for better visualization\n",
    "vasopressor_table = pd.DataFrame(table_data)\n",
    "vasopressor_table.to_csv(f'{output_path}/vasopressor_table_{site_name}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv')\n",
    "\n",
    "vasopressor_table = vasopressor_table.set_index(\"Hospital\")\n",
    "\n",
    "heatmap_data = vasopressor_table[\n",
    "    [\n",
    "       # \"All ICU Readmission (%)\",\n",
    "        \"First ICU: Vasopressor % (12hr since ICU admission)\",\n",
    "        \"First ICU: Vasopressor % (24hr since ICU admission)\",\n",
    "        \"Second ICU: Vasopressor % (12hr since any unplanned ICU readmission)\",\n",
    "        \"Second ICU: Vasopressor % (24hr since any unplanned ICU readmission)\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Create the heatmap with a red color scheme\n",
    "plt.figure(figsize=(14, 10))  # Increased figure size for better layout\n",
    "ax = sns.heatmap(\n",
    "    heatmap_data, \n",
    "    annot=True, \n",
    "    cmap=\"BuPu\",  # Use the \"Reds\" colormap for a red theme\n",
    "    #fmt=\".1f\", \n",
    "    #linewidths=0.5, \n",
    "    #cbar_kws={\"shrink\": 0.8}  # Shrink color bar for better fit\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Vasopressor use by Hospital\", fontsize=16)\n",
    "#plt.tight_layout()\n",
    "plt.xlabel(\"Metrics\", fontsize=12)\n",
    "plt.ylabel(\"Hospital\", fontsize=12)\n",
    "#plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "\n",
    "# Ensure proper layout\n",
    "#plt.tight_layout()  # Automatically adjust elements to fit into the figure\n",
    "#plt.subplots_adjust(left=0.2, right=0.9, top=0.9, bottom=0.2)  # Extra manual adjustments\n",
    "\n",
    "\n",
    "ax.set_ylim(len(heatmap_data) + 0.5, - 0.5)\n",
    "plt.savefig(f'{intermediate_output_path}/vasopressor_heatmap_{site_name}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support device use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by hospital category\n",
    "hospital_groups = icu_final.groupby('hospital_category')\n",
    "\n",
    "# Initialize an empty list to store table rows\n",
    "table_data = []\n",
    "\n",
    "# Loop over each hospital group to collect data\n",
    "for hospital, group in hospital_groups:\n",
    "    # Generate hospital-specific variable prefix\n",
    "    hospital_name = hospital.lower().replace(\" \", \"_\")\n",
    "\n",
    "    # Retrieve previously saved variables\n",
    "    total_unique_visits = globals().get(f\"{hospital_name}_total_unique_visits\", 0)\n",
    "    unplanned_icu_readmission = globals().get(f\"{hospital_name}_unplanned_icu_readmission\", 0)\n",
    "    unplanned_icu_readmission_6hours = globals().get(f\"{hospital_name}_unplanned_icu_readmission_6hours\", 0)\n",
    "    unplanned_icu_readmission_24hours = globals().get(f\"{hospital_name}_unplanned_icu_readmission_24hours\", 0)\n",
    "    unplanned_icu_readmission_48hours = globals().get(f\"{hospital_name}_unplanned_icu_readmission_48hours\", 0)\n",
    "    unplanned_icu_readmission_72hours = globals().get(f\"{hospital_name}_unplanned_icu_readmission_72hours\", 0)\n",
    "\n",
    "    # Current calculations\n",
    "    total_unique_visits = group[\"linked_group\"].nunique()\n",
    "    resp_support_count_12hr = group[(group[\"resp_support_first_12hr\"].isna() == False)][\"linked_group\"].nunique()\n",
    "    resp_support_count_24hr = group[(group[\"resp_support_first_24hr\"].isna() == False)][\"linked_group\"].nunique()\n",
    "    unplanned_firsticu_resp_support_pct_12hr = round((resp_support_count_12hr / total_unique_visits) * 100, 2)\n",
    "    unplanned_firsticu_resp_support_pct_24hr = round((resp_support_count_24hr / total_unique_visits) * 100, 2)\n",
    "\n",
    "    unplanned_secondicu_resp_support_12hr = group[(group[\"procedural\"] == False) & (group[\"resp_support_second_12hr\"].isna() == False)][\"linked_group\"].nunique()\n",
    "    unplanned_secondicu_resp_support_6hr_12hr = group[(group[\"procedural\"] == False) & (group[\"resp_support_second_12hr\"].isna() == False) & (group[\"ICU_readmission_hour\"] < 6)][\"linked_group\"].nunique()\n",
    "    unplanned_secondicu_resp_support_24hr_12hr = group[(group[\"procedural\"] == False) & (group[\"resp_support_second_12hr\"].isna() == False) & (group[\"ICU_readmission_hour\"] < 24)][\"linked_group\"].nunique()\n",
    "    unplanned_secondicu_resp_support_48hr_12hr = group[(group[\"procedural\"] == False) & (group[\"resp_support_second_12hr\"].isna() == False) & (group[\"ICU_readmission_hour\"] < 48)][\"linked_group\"].nunique()\n",
    "    unplanned_secondicu_resp_support_72hr_12hr = group[(group[\"procedural\"] == False) & (group[\"resp_support_second_12hr\"].isna() == False) & (group[\"ICU_readmission_hour\"] < 72)][\"linked_group\"].nunique()\n",
    "    \n",
    "    unplanned_secondicu_resp_support_24hr = group[(group[\"procedural\"] == False) & (group[\"resp_support_second_24hr\"].isna() == False)][\"linked_group\"].nunique()\n",
    "    unplanned_secondicu_resp_support_6hr_24hr = group[(group[\"procedural\"] == False) & (group[\"resp_support_second_24hr\"].isna() == False) & (group[\"ICU_readmission_hour\"] < 6)][\"linked_group\"].nunique()\n",
    "    unplanned_secondicu_resp_support_24hr_24hr = group[(group[\"procedural\"] == False) & (group[\"resp_support_second_24hr\"].isna() == False) & (group[\"ICU_readmission_hour\"] < 24)][\"linked_group\"].nunique()\n",
    "    unplanned_secondicu_resp_support_48hr_24hr = group[(group[\"procedural\"] == False) & (group[\"resp_support_second_24hr\"].isna() == False) & (group[\"ICU_readmission_hour\"] < 48)][\"linked_group\"].nunique()\n",
    "    unplanned_secondicu_resp_support_72hr_24hr = group[(group[\"procedural\"] == False) & (group[\"resp_support_second_24hr\"].isna() == False) & (group[\"ICU_readmission_hour\"] < 72)][\"linked_group\"].nunique()\n",
    "    \n",
    "    unplanned_secondicu_resp_support_pct_12hr = round((unplanned_secondicu_resp_support_12hr / unplanned_icu_readmission) * 100, 2) if unplanned_icu_readmission > 0 else 0\n",
    "    unplanned_secondicu_resp_support_6hr_pct_12hr = round((unplanned_secondicu_resp_support_6hr_12hr / unplanned_icu_readmission_6hours) * 100, 2) if unplanned_icu_readmission_6hours > 0 else 0\n",
    "    unplanned_secondicu_resp_support_24hr_pct_12hr = round((unplanned_secondicu_resp_support_24hr_12hr / unplanned_icu_readmission_24hours) * 100, 2) if unplanned_icu_readmission_24hours > 0 else 0\n",
    "    unplanned_secondicu_resp_support_48hr_pct_12hr = round((unplanned_secondicu_resp_support_48hr_12hr / unplanned_icu_readmission_48hours) * 100, 2) if unplanned_icu_readmission_48hours > 0 else 0\n",
    "    unplanned_secondicu_resp_support_72hr_pct_12hr = round((unplanned_secondicu_resp_support_72hr_12hr / unplanned_icu_readmission_72hours) * 100, 2) if unplanned_icu_readmission_72hours > 0 else 0\n",
    "\n",
    "    unplanned_secondicu_resp_support_pct_24hr = round((unplanned_secondicu_resp_support_24hr / unplanned_icu_readmission) * 100, 2) if unplanned_icu_readmission > 0 else 0\n",
    "    unplanned_secondicu_resp_support_6hr_pct_24hr = round((unplanned_secondicu_resp_support_6hr_24hr / unplanned_icu_readmission_6hours) * 100, 2) if unplanned_icu_readmission_6hours > 0 else 0\n",
    "    unplanned_secondicu_resp_support_24hr_pct_24hr = round((unplanned_secondicu_resp_support_24hr_24hr / unplanned_icu_readmission_24hours) * 100, 2) if unplanned_icu_readmission_24hours > 0 else 0\n",
    "    unplanned_secondicu_resp_support_48hr_pct_24hr = round((unplanned_secondicu_resp_support_48hr_24hr / unplanned_icu_readmission_48hours) * 100, 2) if unplanned_icu_readmission_48hours > 0 else 0\n",
    "    unplanned_secondicu_resp_support_72hr_pct_24hr = round((unplanned_secondicu_resp_support_72hr_24hr/ unplanned_icu_readmission_72hours) * 100, 2) if unplanned_icu_readmission_72hours > 0 else 0\n",
    "    \n",
    "    \n",
    "    # Append a row for the current hospital\n",
    "    table_data.append({\n",
    "        \"Hospital\": hospital,\n",
    "        \"First ICU: Respiratory Support % (12hr since ICU admission)\": unplanned_firsticu_resp_support_pct_12hr,\n",
    "        \"First ICU: Respiratory Support % (24hr since ICU admission)\": unplanned_firsticu_resp_support_pct_24hr,\n",
    "        \"Second ICU: Respiratory Support % (12hr since any unplanned ICU readmission)\": unplanned_secondicu_resp_support_pct_12hr,\n",
    "        \"Second ICU: Respiratory Support % (12hr since <24hr unplanned ICU readmission)\": unplanned_secondicu_resp_support_24hr_pct_12hr,\n",
    "        \"Second ICU: Respiratory Support % (12hr since <48hr unplanned ICU readmission)\": unplanned_secondicu_resp_support_48hr_pct_12hr,\n",
    "        \"Second ICU: Respiratory Support % (12hr since <72hr unplanned ICU readmission)\": unplanned_secondicu_resp_support_72hr_pct_12hr,\n",
    "        \"Second ICU: Respiratory Support % (24hr since any unplanned ICU readmission)\": unplanned_secondicu_resp_support_pct_24hr,\n",
    "        \"Second ICU: Respiratory Support % (24hr since <24hr unplanned ICU readmission)\": unplanned_secondicu_resp_support_24hr_pct_24hr,\n",
    "        \"Second ICU: Respiratory Support % (24hr since <48hr unplanned ICU readmission)\": unplanned_secondicu_resp_support_48hr_pct_24hr,\n",
    "        \"Second ICU: Respiratory Support % (24hr since <72hr unplanned ICU readmission)\": unplanned_secondicu_resp_support_72hr_pct_24hr,\n",
    "    })\n",
    "\n",
    "# Convert the data to a DataFrame for better visualization\n",
    "resp_support_table = pd.DataFrame(table_data)\n",
    "\n",
    "resp_support_table.to_csv(f'{output_path}/resp_support_table_{site_name}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv')\n",
    "\n",
    "resp_support_table = resp_support_table.set_index(\"Hospital\")\n",
    "\n",
    "heatmap_data = resp_support_table[\n",
    "    [\n",
    "       # \"All ICU Readmission (%)\",\n",
    "        \"First ICU: Respiratory Support % (12hr since ICU admission)\",\n",
    "        \"First ICU: Respiratory Support % (24hr since ICU admission)\",\n",
    "        \"Second ICU: Respiratory Support % (12hr since any unplanned ICU readmission)\",\n",
    "        \"Second ICU: Respiratory Support % (24hr since any unplanned ICU readmission)\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Create the heatmap with a red color scheme\n",
    "plt.figure(figsize=(14, 10))  # Increased figure size for better layout\n",
    "ax = sns.heatmap(\n",
    "    heatmap_data, \n",
    "    annot=True, \n",
    "    cmap=\"BuPu\",  # Use the \"Reds\" colormap for a red theme\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Respriatory Support use by Hospital\", fontsize=16)\n",
    "#plt.tight_layout()\n",
    "plt.xlabel(\"Metrics\", fontsize=12)\n",
    "plt.ylabel(\"Hospital\", fontsize=12)\n",
    "\n",
    "ax.set_ylim(len(heatmap_data) + 0.5, - 0.5)\n",
    "plt.savefig(f'{intermediate_output_path}/resp_support_heatmap_{site_name}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Sankey Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_deaths_df2 = dropped_deaths_df.rename(columns={\n",
    "    'in_dttm': 'earliest_location_start',\n",
    "    'out_dttm': 'latest_location_end'\n",
    "})\n",
    "\n",
    "dropped_df_icu_discharge2 = dropped_df_icu_discharge.rename(columns={\n",
    "    'in_dttm': 'earliest_location_start',\n",
    "    'out_dttm': 'latest_location_end'\n",
    "})\n",
    "\n",
    "print(dropped_deaths_df2[\"linked_group\"].nunique())\n",
    "print(dropped_df_icu_discharge2[\"linked_group\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_df7_death = pd.concat([dropped_df_icu_discharge2[[\"patient_id\",\"linked_group\",\"earliest_location_start\",\n",
    "                                                    \"latest_location_end\",\"location_category\",\"procedural\"]], \n",
    "                           dropped_deaths_df2[[\"patient_id\",\"linked_group\",\"earliest_location_start\",\n",
    "                                                    \"latest_location_end\",\"location_category\",\"procedural\"]]], axis=0)\n",
    "icu_df7_death = pd.concat([icu_df7_death, icu_df7[[\"patient_id\",\"linked_group\",\"earliest_location_start\",\n",
    "                                                    \"latest_location_end\",\"location_category\",\"procedural\"]]], axis=0)\n",
    "\n",
    "icu_df7_death = pd.merge(icu_df7_death,patient[[\"patient_id\",\"death_dttm\"]],on=\"patient_id\",how=\"left\")\n",
    "\n",
    "icu_df7_death = pd.merge(icu_df7_death,df_idlist,on=\"patient_id\",how=\"left\")\n",
    "icu_df7_death = pd.merge(icu_df7_death,hosp[[\"hospitalization_id\",\"discharge_category\"]],on=\"hospitalization_id\",how=\"left\")\n",
    "icu_df7_death = pd.merge(icu_df7_death, hospital_block2, on=\"linked_group\", how=\"left\").drop_duplicates()\n",
    "\n",
    "print(\"Number of rows in df:\",len(icu_df7_death))\n",
    "print(\"Number of unique patient_id:\", icu_df7_death[\"patient_id\"].nunique())\n",
    "print(\"Number of unique linked hospitalization_id:\", icu_df7_death[\"linked_group\"].nunique())\n",
    "icu_df7_death.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_deaths(df):\n",
    "    # Ensure the columns are in datetime format\n",
    "    df['death_dttm'] = pd.to_datetime(df['death_dttm'], errors='coerce').dt.tz_localize(None)\n",
    "    df['earliest_location_start'] = pd.to_datetime(df['earliest_location_start'], errors='coerce').dt.tz_localize(None)\n",
    "    df['latest_location_end'] = pd.to_datetime(df['latest_location_end'], errors='coerce').dt.tz_localize(None)\n",
    "\n",
    "    # Create the location_category_death column as a copy of location_category\n",
    "    df['location_category_death'] = df['location_category']\n",
    "\n",
    "    # Sort the dataframe by hospitalization_id and earliest_location_start to ensure order\n",
    "    df = df.sort_values(by=['linked_group', 'earliest_location_start'])\n",
    "\n",
    "    # Vectorized condition to check if death_dttm is between earliest_location_start and latest_location_end\n",
    "    death_condition = (\n",
    "        df['death_dttm'].notna() & \n",
    "        (df['earliest_location_start'] <= df['death_dttm']) & \n",
    "        (df['death_dttm'] <= df['latest_location_end'])\n",
    "    )\n",
    "\n",
    "    # Get unique hospitalization_ids where death occurs during the stay\n",
    "    death_hosp_ids = df.loc[death_condition, 'linked_group'].unique()\n",
    "\n",
    "    # Get the last row of each hospitalization where death occurs\n",
    "    death_rows = df[df['linked_group'].isin(death_hosp_ids)].groupby('linked_group').last().reset_index()\n",
    "\n",
    "    # Modify those rows to represent the 'Death' event\n",
    "    death_rows['location_category_death'] = 'Died'\n",
    "    death_rows['earliest_location_start'] = death_rows['latest_location_end'] + pd.Timedelta(seconds=1)\n",
    "    death_rows['latest_location_end'] = death_rows['earliest_location_start']  # Ensure 'Death' row has valid time window\n",
    "\n",
    "    # Concatenate the new 'Death' rows to the original dataframe\n",
    "    df = pd.concat([df, death_rows], ignore_index=True)\n",
    "\n",
    "    # Sort the dataframe again to ensure 'Death' rows come after the original locations\n",
    "    df = df.sort_values(by=['linked_group', 'earliest_location_start']).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "# Apply the function to the DataFrame\n",
    "icu_df7_death = mark_deaths(icu_df7_death)\n",
    "\n",
    "# Output the results\n",
    "print(\"Number of dead patients:\", icu_df7_death[icu_df7_death[\"location_category_death\"] == \"Died\"][\"linked_group\"].nunique())\n",
    "print(\"Number of rows in df:\", len(icu_df7_death))\n",
    "print(\"Number of unique hospitalization_id:\", icu_df7_death[\"linked_group\"].nunique())\n",
    "icu_df7_death.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_df7_death[icu_df7_death[\"location_category_death\"]==\"Died\"][[\"linked_group\",\n",
    "                                                                  \"earliest_location_start\",\n",
    "                                                                 \"latest_location_end\",\n",
    "                                                                 \"death_dttm\",\n",
    "                                                                 \"location_category\",\n",
    "                                                                 \"location_category_death\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code chunk takes longer to run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_df7_death.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to collapse segments\n",
    "def collapse_segments(df):\n",
    "    df = df.sort_values('earliest_location_start').reset_index(drop=True)\n",
    "    df['is_icu_in_location_category'] = (df['location_category'] == 'ICU')\n",
    "    df['loc_cat_change'] = (df['location_category_death'] != df['location_category_death'].shift(1)).astype(int)\n",
    "    df['segment_id'] = df['loc_cat_change'].cumsum()\n",
    "\n",
    "    grouped = df.groupby(['segment_id', 'location_category_death']).agg({\n",
    "        'earliest_location_start': 'min',\n",
    "        'latest_location_end': 'max',\n",
    "        'is_icu_in_location_category': 'max'\n",
    "    }).reset_index()\n",
    "\n",
    "    grouped['linked_group'] = df['linked_group'].iloc[0]\n",
    "    grouped = grouped[['linked_group', 'segment_id', 'earliest_location_start', 'latest_location_end', 'location_category_death', 'is_icu_in_location_category']]\n",
    "\n",
    "    icu_segments = grouped[grouped['is_icu_in_location_category'] == 1]\n",
    "    if icu_segments.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    first_icu_index = icu_segments.index[0]\n",
    "    grouped['segment_rank'] = None\n",
    "    grouped.loc[first_icu_index:, 'segment_rank'] = range(1, len(grouped.loc[first_icu_index:]) + 1)\n",
    "    grouped['segment_rank'] = grouped['segment_rank'].ffill()\n",
    "\n",
    "    max_segment_rank = grouped['segment_rank'].dropna().max()\n",
    "    grouped['max_segment_rank'] = max_segment_rank\n",
    "    grouped = grouped.drop(columns=['is_icu_in_location_category'])\n",
    "\n",
    "    return grouped\n",
    "\n",
    "# Apply the collapse_segments function using groupby and dictionary comprehension\n",
    "collapsed_adt_dict = {\n",
    "    hospital: group[['linked_group', 'earliest_location_start', 'latest_location_end', 'location_category', 'location_category_death']] \\\n",
    "        .groupby('linked_group', group_keys=False).apply(collapse_segments).reset_index(drop=True)\n",
    "    for hospital, group in tqdm(icu_df7_death[icu_df7_death[\"procedural\"] == False].groupby('hospital_category'), desc=\"Processing Hospitals\")\n",
    "}\n",
    "\n",
    "# Save the collapsed DataFrames as globals\n",
    "for hospital, collapsed_adt in collapsed_adt_dict.items():\n",
    "    globals()[f\"{hospital.lower().replace(' ', '_')}_collapsed_adt\"] = collapsed_adt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop intermediate ER rows\n",
    "def drop_intermediate_er2(df):\n",
    "    df = df.sort_values(by=['linked_group', 'earliest_location_start', 'latest_location_end'])\n",
    "    df['is_first_row'] = df.groupby('linked_group').cumcount() == 0\n",
    "\n",
    "    er_after_first_row = df.groupby('linked_group').apply(\n",
    "        lambda group: (group.loc[~group['is_first_row'], 'location_category_death'] == 'ER').any()\n",
    "    )\n",
    "\n",
    "    visits_to_drop = er_after_first_row[er_after_first_row].index\n",
    "    df_cleaned = df[~df['linked_group'].isin(visits_to_drop)]\n",
    "    dropped_df = df[df['linked_group'].isin(visits_to_drop)]\n",
    "    df_cleaned = df_cleaned.drop(columns=['is_first_row'])\n",
    "\n",
    "    return df_cleaned, dropped_df\n",
    "\n",
    "# Apply the drop_intermediate_er2 function using dictionary comprehensions\n",
    "final_results = {\n",
    "    hospital: drop_intermediate_er2(collapsed_adt)\n",
    "    for hospital, collapsed_adt in tqdm(collapsed_adt_dict.items(), desc=\"Dropping Intermediate ER Rows\")\n",
    "}\n",
    "\n",
    "# Extract cleaned and dropped DataFrames\n",
    "final_cleaned_dict = {hospital: result[0] for hospital, result in final_results.items()}\n",
    "final_dropped_dict = {hospital: result[1] for hospital, result in final_results.items()}\n",
    "\n",
    "# Save the cleaned and dropped DataFrames as globals\n",
    "for hospital, cleaned in final_cleaned_dict.items():\n",
    "    globals()[f\"{hospital.lower().replace(' ', '_')}_cleaned_adt\"] = cleaned\n",
    "\n",
    "for hospital, dropped in final_dropped_dict.items():\n",
    "    globals()[f\"{hospital.lower().replace(' ', '_')}_dropped_adt\"] = dropped\n",
    "\n",
    "# Print results\n",
    "for hospital, cleaned in final_cleaned_dict.items():\n",
    "    print(f\"Hospital: {hospital}\")\n",
    "    print(f\"  Number of unique linked hospitalization_id in cleaned: {cleaned['linked_group'].nunique()}\")\n",
    "    print(f\"  Number of rows in cleaned DataFrame: {len(cleaned)}\")\n",
    "    print(f\"  Number of rows in dropped DataFrame: {len(final_dropped_dict[hospital])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to propagate 'Died' across segments\n",
    "def propagate_death(df):\n",
    "    segment_cols = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
    "\n",
    "    def propagate(row):\n",
    "        death_found = False\n",
    "        for col in segment_cols:\n",
    "            if row[col] == 'Died':\n",
    "                death_found = True\n",
    "            if death_found:\n",
    "                row[col] = 'Died'\n",
    "        return row\n",
    "\n",
    "    df[segment_cols] = df[segment_cols].apply(propagate, axis=1)\n",
    "    return df\n",
    "\n",
    "# Define colors for the Sankey diagram\n",
    "colors = {\n",
    "    \"ICU\": 'lightcoral',\n",
    "    \"Ward\": 'skyblue',\n",
    "    'Procedural': 'thistle',\n",
    "    \"Discharged\": 'lightgrey',\n",
    "    \"Died\": 'grey',\n",
    "    \"ER\": 'red'\n",
    "}\n",
    "\n",
    "# Output path for saving figures\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Iterate over each hospital's collapsed_adt2 DataFrame and generate Sankey diagrams\n",
    "for hospital, collapsed_adt2 in tqdm(final_cleaned_dict.items(), desc=\"Generating Sankey Diagrams\"):\n",
    "    sankey_df = collapsed_adt2.reset_index().loc[:, ['linked_group', 'segment_rank', 'location_category_death']]\n",
    "    sankey_df2 = sankey_df.loc[sankey_df.segment_rank.notna()]\n",
    "    sankey_df3 = sankey_df2.pivot(\n",
    "        index=['linked_group'],\n",
    "        columns='segment_rank',\n",
    "        values='location_category_death'\n",
    "    ).reset_index().fillna('Discharged')\n",
    "\n",
    "    sankey_df4 = sankey_df3.iloc[:, :8]  # Only take first 7 locations\n",
    "\n",
    "    # Apply the function to propagate 'Died'\n",
    "    sankey_df4 = propagate_death(sankey_df4)\n",
    "\n",
    "    # Create the Sankey diagram\n",
    "    fig, ax = plt.subplots(figsize=(14, 6), constrained_layout=True)\n",
    "    diag = Sankey(\n",
    "        sankey_df4.iloc[:, 1:],\n",
    "        ax=ax,\n",
    "        order=[\"ICU\", \"Ward\", \"Procedural\", \"Discharged\", \"ER\", \"Died\"],\n",
    "        block_width=0.2,\n",
    "        colors=colors,\n",
    "        alpha=0.5\n",
    "    )\n",
    "    diag.draw()\n",
    "    ax.set_title(f\"Sankey Diagram for {hospital}\", size=16)\n",
    "    ax.set_xticks([\n",
    "        diag.block_width / 2 + diag.flow_width * x + diag.block_width * x \n",
    "        for x in range(sankey_df4.shape[1] - 1)\n",
    "    ])\n",
    "    ax.set_xticklabels(sankey_df4.columns[1:].astype(int))\n",
    "    ax.set_xlabel(\"Location number\", size=14)\n",
    "    ax.get_xaxis().set_visible(True)\n",
    "    ax.tick_params(axis=\"x\", pad=5, labelsize=16)\n",
    "\n",
    "    # Save the figure\n",
    "    fig.savefig(\n",
    "        f'{output_path}/sankey_diagram_{hospital.lower().replace(\" \", \"_\")}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.png',\n",
    "        dpi=300,\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Table One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unplanned_icu_final = icu_final[icu_final[\"procedural\"]==False]\n",
    "#admission location\n",
    "admission_location = icu_df7[[\"linked_group\", \"location_category\"]].groupby('linked_group', as_index=False).first()\n",
    "unplanned_icu_final = pd.merge(unplanned_icu_final,admission_location,on=\"linked_group\",how=\"left\")\n",
    "\n",
    "#length of ICU stay \n",
    "icu_sorted = icu_df7.sort_values(by=['linked_group', 'earliest_location_start'])\n",
    "icu_first_icu = icu_sorted[icu_sorted['location_category'] == 'ICU'].groupby('linked_group', as_index=False).first()\n",
    "length_ICU = icu_first_icu[['linked_group', 'location_hours']]\n",
    "unplanned_icu_final = pd.merge(unplanned_icu_final,length_ICU,on=\"linked_group\",how=\"left\")\n",
    "unplanned_icu_final['location_days'] = unplanned_icu_final['location_hours'] / 24\n",
    "\n",
    "#in-hospital mortality\n",
    "collapsed_adt2_concat = pd.concat([collapsed_adt2 for hospital, collapsed_adt2 in tqdm(final_cleaned_dict.items())], ignore_index=True)\n",
    "dead_patients = collapsed_adt2_concat[collapsed_adt2_concat[\"location_category_death\"]==\"Died\"][[\"linked_group\",\"location_category_death\"]]\n",
    "unplanned_icu_final = pd.merge(unplanned_icu_final,dead_patients,on=\"linked_group\",how=\"left\")\n",
    "unplanned_icu_final['location_category_death'] = unplanned_icu_final['location_category_death'].fillna('Alive')\n",
    "\n",
    "#age at admission\n",
    "hosp = hosp.drop_duplicates(subset=[\"hospitalization_id\"])\n",
    "age = hosp[[\"hospitalization_id\",\"age_at_admission\"]] \n",
    "unplanned_icu_final = pd.merge(unplanned_icu_final,df_idlist,on=\"patient_id\",how=\"left\")\n",
    "unplanned_icu_final = pd.merge(unplanned_icu_final,age,on=\"hospitalization_id\",how=\"left\")\n",
    "\n",
    "#demographics \n",
    "demog = patient[[\"patient_id\",\"race_category\",\"ethnicity_category\",\"sex_category\"]]\n",
    "unplanned_icu_final = pd.merge(unplanned_icu_final,demog,on=\"patient_id\",how=\"left\")\n",
    "unplanned_icu_final = unplanned_icu_final.drop_duplicates(subset=[\"linked_group\"])\n",
    "\n",
    "#readmitted\n",
    "unplanned_icu_final['readmission'] = unplanned_icu_final['ICU_readmission_hour'].apply(\n",
    "    lambda x: 'Readmitted' if pd.notna(x) and x > 0 else 'Not Readmitted'\n",
    ")\n",
    "\n",
    "print(unplanned_icu_final[\"linked_group\"].nunique())\n",
    "unplanned_icu_final.to_csv(f'{intermediate_output_path}/unplanned_icu_final_{site_name}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv')\n",
    "unplanned_icu_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_one_dat = unplanned_icu_final[[\"age_at_admission\",\n",
    "                                     \"sex_category\",\n",
    "                                     \"race_category\",\n",
    "                                    \"ethnicity_category\",\n",
    "                                    \"location_category\",\n",
    "                                     \"ICU_readmission_hour\",\n",
    "                                    \"location_days\",\n",
    "                                    \"location_category_death\",\n",
    "                                    \"readmission\",\n",
    "                                    'hospital_category']]\n",
    "\n",
    "# Rename columns\n",
    "table_one_dat = table_one_dat.rename(columns={'age_at_admission': 'Age', \n",
    "                                              'sex_category': 'Sex', \n",
    "                                              'race_category': 'Race',\n",
    "                                             'ethnicity_category': 'Ethnicity',\n",
    "                                             'location_category': 'Initial hospital location',\n",
    "                                              'ICU_readmission_hour':'ICU readmission, hours',\n",
    "                                              \"location_days\":'ICU length of stay, days',\n",
    "                                              \"location_category_death\":'In-hospital Mortality'\n",
    "                                             })\n",
    "\n",
    "\n",
    "# Define the columns that include both continuous and categorical variables\n",
    "columns = [\"Age\", \"ICU readmission, hours\", \"ICU length of stay, days\", \"Sex\", \"Race\", \"Ethnicity\", \"Initial hospital location\", \"In-hospital Mortality\"]\n",
    "\n",
    "# Define which columns are categorical\n",
    "categorical = [\"Sex\", \"Race\", \"Ethnicity\", \"Initial hospital location\", \"In-hospital Mortality\"]\n",
    "\n",
    "# Define which continuous variables are not normally distributed\n",
    "nonnormal = [\"Age\",\"ICU readmission, hours\",\"ICU length of stay, days\"]\n",
    "\n",
    "#groupby\n",
    "groupby = 'readmission'\n",
    "\n",
    "# Loop over each hospital_category\n",
    "for hospital_category in table_one_dat['hospital_category'].unique():\n",
    "    # Filter data for the specific hospital category\n",
    "    hospital_data = table_one_dat[table_one_dat['hospital_category'] == hospital_category]\n",
    "    \n",
    "    # Generate TableOne for the hospital\n",
    "    table1 = TableOne(data=hospital_data, \n",
    "                      columns=columns, \n",
    "                      categorical=categorical, \n",
    "                      #nonnormal=nonnormal, \n",
    "                      groupby=groupby)\n",
    "    \n",
    "    # Save the table to a CSV file\n",
    "    site_name = hospital_category.replace(\" \", \"_\").lower()  # Create a file-safe site name\n",
    "    table1.to_csv(f'{output_path}/table1_{site_name}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv')\n",
    "    print(f\"TableOne for {hospital_category} saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
